\chapter{内存管理}
\begin{bibunit}[ieeetr]

\section{实验说明}
在之前的实验中，我们已经完成了进程的管理和通信，并实现了例外的处理，使得我们的操作系统可以正确的运行一个或多个进程。但是，细心的同学们可能会注意到，我们每个进程的地址空间需要事先通过编译确定，这样多个进程才能同时在一个地址空间内运行而不互相干扰。

但是，如何事先就预知有哪些进程会在一起运行呢？能不能动态的加载进程到任意一个空闲的地址区域内，并且进程间不会互相干扰呢？虚拟内存机制就能帮我们做到这一件事。我们可以给每个进程一个“虚拟”的地址空间，每个进程认为自己能访问到的地址空间都是一样的，并且互相不会干扰。

此外对于操作系统来说，安全也是一个重要的功能。系统的数据安全必须要靠数据隔离来实现，而基于虚拟内存的内存管理，也是操作系统中用来确保数据隔离的重要机制。

在本实验中，我们将学习操作系统的虚拟内存管理机制，包括虚实地址空间的管理，换页机制等。
请同学们认真思考各部分的设计，考虑操作系统的安全性和性能，完成好虚拟内存管理的功能。

本次实验的各个任务如下，做S-core的同学需要完成任务一，A-core的同学需要完成任务一至任务四，C-core的同学需要完成所有任务：

\begin{description}
    \item[任务一] 启用虚拟内存机制进入内核，实现内存隔离机制，从SD卡中加载用户程序，使用户进程可以使用虚拟地址访问内存。
    \item[任务二] 实现缺页处理程序，发生缺页（page fault）异常时自动分配物理页面。并验证之前的进程锁实现在虚拟内存机制开启的情况下依然有效。
    \item[任务三] 实现换页机制，在物理内存不够时或者当物理页框不在内存中时，将数据与SD卡之间进行交换，从而支持将虚拟地址空间进一步扩大。
    \item[任务四] 实现虚拟内存上的多线程管理，使得一个进程可以用多个线程分别执行不同的任务。
    \item[任务五] 实现共享内存机制，使得两个进程可以使用共享的一块物理内存，并用共享内存机制完成进程通信。
    % \item[任务六] 实现copy-on-write策略，并编写带有fork功能的测试程序进行验证。
    \item[任务六] 实现\texttt{mprotect}系统调用，实现对指定内存范围的只读、只写、不可执行。测试程序尝试访问这一段。
    \item[任务七] 实现\texttt{brk/sbrk}系统调用，向库函数添加\texttt{malloc()/free()}，限制用户自由使用的地址空间范围。
    
    % \item [任务五] 实现网卡驱动以及文件系统在虚拟内存下的集成
\end{description}

各个core的同学需要完成的任务如下表所示。另外，欲完成C-core的同学需要全程使用双核运行操作系统。

\begin{table}[H]
    \centering
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        评分等级 & 需要完成的任务            \\ \midrule
        S-core & 启用虚拟内存机制进入内核，实现虚拟内存上的用户进程启动和静态页表  \\
        A-core & 实现多线程创建和管理，实现缺页中断处理、按需调页和换页机制 \\
        % C-core & 实现共享内存机制，实现copy-on-write策略   \\ \bottomrule
        % C-core & 实现虚拟内存机制下的网卡驱动以及文件系统 \\ \bottomrule
        C-core & 实现共享内存机制，实现\texttt{mprotect}系统调用，实现\texttt{brk/sbrk}系统调用及\texttt{malloc()/free()}库函数 \\ \bottomrule
    \end{tabularx}
    \caption{各个等级需要完成的任务列表}
    \label{tab:core-list}
\end{table}


% RISC-V手册对于虚拟内存的硬件工作机制有详细描述，请大家注意查看关于satp寄存器的相关说明，以及supervisor态下的虚拟内存机制的说明。



\section{本章解读}

这一部分的要点是：

\begin{enumerate}
    \item 理解RISC-V处理器的虚拟内存机制
    \item 理解页表的基本原理和实现
\end{enumerate}

本章最重要的就是\textbf{理解地址是怎么映射的}。
说简单点，其实虚拟内存机制就是一套虚拟地址到物理地址的映射。快表（TLB）相当于页表的高速缓存。
那么页表是什么样的结构呢？一说到映射，很多人想的可能是一个页表项需要存储一个虚地址一个物理地址，然后每次查整张表匹配虚地址，然后再找到对应的物理地址。
但实际上，\textbf{页表只存物理地址}。\textbf{你可以把页表想想成一个数组，数组的下标是虚地址，数组的内容是下标所对应的物理地址}。
当然，虚地址空间很大，而且可能很多我们都不会用到，所以可以采用多级页表，每次索引虚地址的几位，查到下一级页表的物理地址，直到查到最后一级页表，页表项里面存的才是对应的物理地址。

最后，再次强调一下本章的\textbf{核心要点}：\textbf{页表存储的都是物理地址，处理器访问的都是虚地址}。
    
    
    
\section{启用虚拟内存机制}

说到内存，同学们想必已经不再陌生，不仅是因为每台计算机中都有内存，
而且同学们在之前的Project中，已经知道了我们操作系统的 boot block 会放在\c{0x50200000}的位置，并且能将用户程序加载到对应的内存地址并执行。
不仅如此，同学们在调试中可能也会偶尔发生系统报告 page fault 的错误，报这样的错是因为访问到了\texttt{0x50000000- 0x60000000}之外的地址。

内存中保存了程序所需的所有代码和数据，其内容不能被随意篡改，也不应该被其他的程序随意访问。
因此，安全性是操作系统最重要的功能之一，在进行操作系统设计时必须予以考虑。
通过理论课的学习，我们已经了解到，操作系统通过虚拟内存的机制来实现对内存数据的保护，
但是我们研讨课所编写的操作系统到目前为止显然还没有这样的功能，
所以在本实验的第一个任务中，我们就先把操作系统最基本的虚拟内存机制建立起来。

\subsection{虚拟内存的基本概念}

虽然大家理论课已经学过，但这里为了便于理解还是简述一下虚拟内存的概念。
大家可以回忆一下前面我们的用户测试程序的加载方法：把内存当成巨大的数组，每一个元素是一大片可供程序存放的空间，每个任务加载进内存后，都占用了其中一个元素。
这种方法会造成一个棘手的问题，即若某个程序访问、修改了其他程序的内存，可能会读取到错误数据、影响其他程序运行，带来严重的安全隐患。

%\begin{enumerate}
%\item 第一，我们目前所有执行的任务是预先准备好的。任务中的所有的变量的地址、代码中涉及到的地址，全部是在编译期间算好的，且每个任务同一时间只能执行一个。
%想象一下，假如执行一个任务（含有全局变量）时，又启动了一个同样的任务，那么这两个任务使用着相同的全局变量。
%在我们目前设计的内核中，同一个全局变量只有一个地址。所以这两个任务肯定都会去修改这个全局变量，导致执行错误。
%\item 第二，假如有一个编写得不好的任务，不小心把别人的用户栈甚至是内核栈覆盖了，那么整个系统就直接崩溃了。
%\end{enumerate}

为了解决这个问题，人们设计了虚拟内存机制。
我们之前实现的进程调度机制能够让每个进程都认为自己独享了整个CPU，
但实际上它们只是占用了CPU的一些时间片，并没有完全独占CPU，而是在与分时共享。虚拟内存的思想与之类似：每个进程都自认为独享了整个内存，但实际上它们只是分享了物理内存的不同部分。
CPU共享中的基本单位是时间片，而物理内存则被分割成不同的页以分享。
虚拟内存管理机制将物理内存切分为固定大小的\textbf{页框 (page frame)}，并送给不同进程，支撑起进程的内存地址空间。
为了禁止跨进程的内存访问、给不同进程提供同样的内存环境，进程访问的地址并不是真正的物理地址，而是内存管理程序为它提供的虚拟地址。
地址空间即一个可以被地址索引的存储空间。我们为每个进程提供一个独立的虚拟地址空间，勒令进程仅能访问此空间内的地址。
同时我们按照页的大小（和页框的大小一致）分割虚拟地址空间，设置其与物理内存空间之间的对应关系。
这样当程序访问一个虚拟地址的时候，若其有对应的物理地址，操作系统和处理器就可以完成地址换算，进行访存操作。
由虚拟地址向物理地址的翻译通常由处理器自动完成。

记录虚拟地址页与物理页框对应关系的数据结构就叫\textbf{页表 (page table)}。

\subsection{内存控制相关的CSR}

内存管理单元MMU（Memory Management Unit）是负责处理CPU中访存请求的计算机硬件，它需要处理虚拟地址到物理地址转换的计算与设置（内存地址翻译、虚拟内存管理）、内存保护、CPU cache的控制，而我们主要关心它与虚实地址转换的关联。

在RISC-V指令集规定下，虚拟内存管理默认关闭，CPU的访存操作地址会直接送往物理内存。Supervisor特权级的\texttt{satp}寄存器可以控制分页机制，若启用，在S/U特权级下的访存操作地址会被视为虚拟地址，会被MMU地址翻译成物理地址再送往物理内存进行后续操作。

MMU会根据\texttt{satp}寄存器中记录的物理地址去查找页目录，发现整套页表，并依据虚拟地址检索对应的页表项，得到实地址，完成内存地址翻译。
\texttt{satp}寄存器的结构如图\ref{fig:rv64satp}所示。

\begin{figure}[hbtp!]
    {\footnotesize
        \begin{center}
            \begin{tabular}{@{}S@{}T@{}U}
                \instbitrange{63}{60} &
                \instbitrange{59}{44} &
                \instbitrange{43}{0} \\
                \hline
                \multicolumn{1}{|c|}{{\tt MODE} (\warl)} &
                \multicolumn{1}{|c|}{{\tt ASID} (\warl)} &
                \multicolumn{1}{|c|}{{\tt PPN}  (\warl)} \\
                \hline
                4 & 16 & 44 \\
            \end{tabular}
        \end{center}
    }
    \vspace{-0.1in}
    \caption{RV64 Supervisor address translation and protection register {\tt satp}, for MODE
    values Bare, Sv39, and Sv48.\cite{riscv-privileged}}
    \label{fig:rv64satp}
\end{figure}

\begin{itemize}
    \item \texttt{PPN} 代表页目录位置的物理页框号。页目录的起始地址必须按4KB对齐。
    \item \texttt{ASID} 表示当前地址空间的ID。每个进程都有自己独立的地址空间，这个标志位能够用来区分不同进程。
    \item \texttt{MODE} 表示当前的地址翻译模式，其编码格式如表\ref{tab:satp-mode}所示。
\end{itemize}

\begin{table}[hbtp!]
    \begin{center}
        \begin{tabular}{|c|c|l|}
            \hline
            \multicolumn{3}{|c|}{RV64} \\
            \hline
            Value  & Name & Description \\
            \hline  
            0       & Bare  & No translation or protection. \\
            1--7    & ---   & {\em Reserved} \\
            8       & Sv39  & Page-based 39-bit virtual addressing. \\
            9       & Sv48  & Page-based 48-bit virtual addressing. \\
            10      & {\em Sv57} & {\em Reserved for page-based 57-bit virtual addressing.} \\
            11      & {\em Sv64} & {\em Reserved for page-based 64-bit virtual addressing.} \\
            12--15  & ---   & {\em Reserved} \\
            \hline
        \end{tabular}
    \end{center}
    \caption{Encoding of {\tt satp} MODE field.\cite{riscv-privileged}}
    \label{tab:satp-mode}
\end{table}

代码框架中的 \texttt{arch/riscv/include/pgtable.h} 头文件包含了Sv39地址翻译模式需要用到的宏定义，其中的\texttt{set\_satp()}函数可以用于设置\texttt{satp}寄存器。该函数有三个参数，分别对应着\texttt{satp}的\texttt{MODE}，\texttt{ASID}以及\texttt{PPN}。如下代码可启用Sv39虚拟内存模式，并将页目录设置为\texttt{0x51000000}地址处的物理页框。

\begin{lstlisting}[language=c]
  // satp_MODE_SV39为8，ASID为0，NOMAL_PAGE_SIZE为12，代表4KB的偏移
  // 这些宏定义定义在pgtable.h
  set_satp(satp_MODE_SV39, 0, 0x51000000 >> NORMAL_PAGE_SHIFT);
\end{lstlisting}

\subsection{页框（page frame）}

虚拟内存机制的核心是分页机制。
如图\ref{fig:pageframe}所示，分页机制设定的虚拟/物理地址空间都被划分为固定大小的页框，两者之间的对应关系以页为单位设定。
为构建分页机制，同学们需要将物理地址空间划分为一个个页框。
但是同学们并不需要用程序来特地划分，只需要在设计中了解这一点。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.4\textwidth]{pageframe}
    \caption{分页机制和页框}
    \label{fig:pageframe}
\end{figure}

% 此外，\textbf{还需要注意一个很关键的汇编指令，叫做sfence.vma。
% 这个指令是用来刷新TLB的。因为TLB是硬件自动管理的，当内存中页表项的状态发生变化时，可能出现没有刷新TLB而导致的问题。
% 当我们希望清空TLB时，可以使用该汇编指令。我们已经封装好了基于它的local\_flush\_tlb系列内联函数，可供使用。}
% 另外需要注意，如果对地址映射进行了修改，必要的时候还需要用fence和fence.i刷新数据cache和指令cache。

\subsection{地址格式}

RISC-V一共支持Sv32、Sv39和Sv48这三种虚拟内存管理模式，它们分别支持32位、39位和48位虚地址，需要采用二级、三级、四级页表进行软/硬件实现。

我们的操作系统将在64位的RISC-V处理器上启动，虽然此时虚拟地址位宽应为64位，但是启用Sv32/39/48分页模式时，只有对应的低32/39/48位有意义。
以SV39为例，规定中虚拟地址的\texttt{[63:39]}这几位必须和第 38 位相同，否则 MMU 会直接认定它是一个不合法的虚拟地址。
通过此检查后，MMU 会取出低 39 位，并尝试将其转化为一个 56 位的物理地址（数字56的来历会在下文提及）。

由于我们实验环境配备的物理内存只有256MB大小，28位地址就能够索引全部的物理地址空间，实现四级页表过于复杂且必要性不大，所以我们选择让同学们实现Sv39模式，采用三级页表进行索引。
Sv39支持的虚地址和物理地址格式如图\ref{fig:sv39va}、图\ref{fig:sv39pa}所示\cite{riscv-privileged}。
在分页管理的模式下，我们规定实验中的页大小为$2^{12}=4\mathrm{KB}$，每个虚拟页面和物理页框的地址都需要按页对齐。
一个内存地址因页大小自然地分为两半：页号和页内偏移。
\texttt{[11:0]}是页内偏移 (Page Offset)，是地址于其所在页面中的相对位置。
虚拟地址的\texttt{[38:12]}是虚拟页号 (VPN, Virtual Page Number)，物理地址\texttt{[55:12]}是它的物理页号 (PPN, Physical Page Number)，用来定位地址所属的页面/页框。

\begin{figure}[htbp!]
    {\footnotesize
        \begin{center}
            \begin{tabular}{@{}O@{}O@{}O@{}O}
                \instbitrange{38}{30} &
                \instbitrange{29}{21} &
                \instbitrange{20}{12} &
                \instbitrange{11}{0} \\
                \hline
                \multicolumn{1}{|c|}{VPN[2]} &
                \multicolumn{1}{c|}{VPN[1]} &
                \multicolumn{1}{c|}{VPN[0]} &
                \multicolumn{1}{c|}{page offset} \\
                \hline
                9 & 9 & 9 & 12 \\
            \end{tabular}
        \end{center}
    }
    \vspace{-0.1in}
    \caption{Sv39虚地址}
    \label{fig:sv39va}
\end{figure}

\begin{figure}[hbtp!]
    {\footnotesize
        \begin{center}
            \begin{tabular}{@{}T@{}O@{}O@{}O}
                \instbitrange{55}{30} &
                \instbitrange{29}{21} &
                \instbitrange{20}{12} &
                \instbitrange{11}{0} \\
                \hline
                \multicolumn{1}{|c|}{PPN[2]} &
                \multicolumn{1}{c|}{PPN[1]} &
                \multicolumn{1}{c|}{PPN[0]} &
                \multicolumn{1}{c|}{page offset} \\
                \hline
                26 & 9 & 9 & 12 \\
            \end{tabular}
        \end{center}
    }
    \vspace{-0.1in}
    \caption{Sv39物理地址}
    \label{fig:sv39pa}
\end{figure}

简单来说，地址翻译的就是MMU从39位的虚地址中，取出27位VPN作为“索引”，在页表中查询对应的44位PPN，如果成功的查找到了，则将44位PPN与12位offset拼接成物理地址，否则会产生翻译错误。

页框是管理物理内存的基本单元，页框的大小决定了物理内存分配的粒度。
虽然现有的大部分计算机系统将页框被设置为4KB，但它们也支持粗粒度的内存分配，具体方法为在分页机制中选择性放弃一级或多级页表，转而将中间产生的页作为更大的页使用，这一点会在后文中提到。
% 在本实验中，我们推荐同学们以4KB作为用户态页框大小，当然同学们也可以\textbf{自由选择页框的大小}，既可以使用单一的页框大小，也可以使用混合的页框大小。
% 请采用混合页框大小设计的同学们思考，不同的页框大小对系统的性能和资源使用有什么影响？
% 如果使用混合页框大小，应该如何进行不同大小页面的分配？

\subsection{Sv39页表项说明}

页表就用来存储从虚拟页面到物理页框的对应关系（由于并非所有虚拟页都对应了物理页框，所以这类似于一个偏函数）。
作为一个数据结构，页表本身也需要占用内存的一块空间（为了内存管理，这块内存可以是一些物理页框）。
在本任务中，我们需要在内核初始化时划分一块\textbf{物理页框}用于存放页表。
页表中的每一个条目都是一个页表项 (page table entry, PTE)，保存了虚拟地址到物理地址的映射关系。其抽象数据结构如图\ref{fig:sv39pte}所示。页表项中除了保留的最高数位（为Sv48所保留），包含了 PPN (\texttt{[53:10]}) 和标志着页面的8个不同属性的不同标志位 (\texttt{[7:0]}位)，具体说明如下：
% 图\ref{fig:page-table-translate}所示的就是通过页表项进行虚实地址转换的一个过程。

\begin{figure}[hbtp!]
    {\footnotesize
        \begin{center}
            \begin{tabular}{@{}Y@{}Y@{}Y@{}Y@{}Fcccccccc}
                \instbitrange{63}{54} &
                \instbitrange{53}{28} &
                \instbitrange{27}{19} &
                \instbitrange{18}{10} &
                \instbitrange{9}{8} &
                \instbit{7} &
                \instbit{6} &
                \instbit{5} &
                \instbit{4} &
                \instbit{3} &
                \instbit{2} &
                \instbit{1} &
                \instbit{0} \\
                \hline
                \multicolumn{1}{|c|}{\it Reserved} &
                \multicolumn{1}{c|}{PPN[2]} &
                \multicolumn{1}{c|}{PPN[1]} &
                \multicolumn{1}{c|}{PPN[0]} &
                \multicolumn{1}{c|}{RSW} &
                \multicolumn{1}{c|}{D} &
                \multicolumn{1}{c|}{A} &
                \multicolumn{1}{c|}{G} &
                \multicolumn{1}{c|}{U} &
                \multicolumn{1}{c|}{X} &
                \multicolumn{1}{c|}{W} &
                \multicolumn{1}{c|}{R} &
                \multicolumn{1}{c|}{V} \\
                \hline
                10 & 26 & 9 & 9 & 2 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
            \end{tabular}
        \end{center}
    }
    \vspace{-0.1in}
    \caption{Sv39页表项}
    \label{fig:sv39pte}
\end{figure}


\begin{itemize}

    \item \texttt{R} (Read)、\texttt{W} (Write)和\texttt{X} (eXecute) 分别代表页表项对应的页面是否有读/写/执行权限。

    \item \texttt{G} (Global) 代表页面是否全局可用。

    对于全局可用的页，访问的时候会忽略\texttt{ASID}的标记。这个标志位可以便利一些全局页表项与TLB的交互。本实验中可以不关注其功能，但对于非叶子结点的页目录，请将这一项置0。

    \item \texttt{A} (Access) 和\texttt{D} (Dirty) 分别可以用来代表页面的访问、脏数据性质，可以用来表示页面是否被访问、修改过。\textbf{对于非叶子结点的页目录，请将这两项置0。}

    这两位的具体行为取决于硬件的实现。硬件可以在一个页被访问/写入时自动将A/D置位，也可以直接产生缺页异常。

    我们实验中使用的FPGA开发板上的RISC-V核按后者实现。
    如果\texttt{A}为0且发生了对这个页面的访问、\texttt{D}为0且发生了对这个页面的写入，都会直接触发缺页异常（写入也算内存访问的一种）。

    \item \texttt{U} (User) 代表用户权限。该位为0时，于User特权级对此页面的访问会触发缺页异常。如果\texttt{sstatus}寄存器的\texttt{SUM}位为1，则Supervisor态下也可访问\texttt{U}位为1的页面。\textbf{对于非叶子结点的页目录，请将这一项置0。}

    \item \texttt{V}代表Valid位，\texttt{V}被置位时该页表项有效。
\end{itemize}

\textbf{当系统出现缺页异常时，需要同学们考虑上述bit位未设置正确的情况。}


\subsection{页表与地址转换过程}

\subsubsection{多级页表}

页表一般按页大小存储，以便内存管理。
根据上面的设计，本实验中一个页表项需要8B存储空间，一个4KB的页表可以容纳$512=2^9$个页表项。与此同时，我们设计\texttt{VPN/PPN}为9位，将其作为索引，用于在页表中定位一个页表项。

随着页表项中的\texttt{X/W/R}位的不同，页表的含义也有不同，这与叶子节点（末级页表）与非叶子节点（页目录表，非末级页表）有关，具体如表\ref{tab:rwx_mean}所示。倘若\texttt{R/W/X}位均为0，则标志着该节点为非叶子节点，否则为叶子节点。叶子节点需要将\texttt{R/W/X}位按相应权限设置。

\begin{table}[hbtp!]
  \begin{center}
  \begin{tabular}{|c|c|c|l|}
  \hline
  X & W & R & Meaning \\
  \hline  
  0 & 0 & 0 & Pointer to next level of page table. \\
  0 & 0 & 1 & Read-only page. \\
  0 & 1 & 0 & Reserved for future use. \\
  0 & 1 & 1 & Read-write page. \\
  1 & 0 & 0 & Execute-only page. \\
  1 & 0 & 1 & Read-execute page. \\
  1 & 1 & 0 & Reserved for future use. \\
  1 & 1 & 1 & Read-write-execute page. \\
  \hline
  \end{tabular}
  \end{center}
  \caption{Encoding of PTE R/W/X fields.\cite{riscv-privileged}}
  \label{tab:rwx_mean}
\end{table}

叶子节点中需要记录512个页表项，表项中的PPN与虚拟地址的offset拼接即可得到地址翻译结果。非叶子节点中功能上只需要记录512个下一级节点的位置。鉴于页表节点的内存对齐属性，我们可以以与叶子节点相同的格式保存512个页表项，以拼接后的物理地址代表下一级页表的起始地址，这样我们便可以实现非叶子节点存储下一级页表的功能。

\subsubsection{地址转换过程}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{Sv39vptran}
    \caption{Sv39虚实地址转换过程\cite{book-riscv-rev3}}
    \label{fig:Sv39vptran}
\end{figure}

Sv39模式下三级页表的地址转换过程如图\ref{fig:Sv39vptran}所示。
\textbf{在下文中，我们用第三/二/一级页表，分别指代访存次数意义上距离\texttt{satp}寄存器由近及远排序的三个页表层级（即图中自左向右的三个层级}。
这个地址翻译过程在访存时会由硬件自动完成，软件只需为新进程在内存中设置好这三级页表与\texttt{satp}寄存器。对于一个39位的虚拟地址 \texttt{vaddr = \{VPN2[38:30], VPN1[29:21], VPN0[20:12], offset[11:0]\}}，地址翻译的过程大致如下：

\begin{itemize}
    \item 操作系统已经记录好了“当前应使用的第三级页表”的PPN到 \texttt{satp} 寄存器中；
    \item 以\texttt{VPN2}作为索引值，在第三级页表中找到对应的第二级页表页表项，获得其中PPN，用\texttt{PPN << 12}作为第二级页表的物理地址；
    \item 以\texttt{VPN1}作为索引值，在第二级页表中找到对应的第一级页表页表项，获得其中PPN，用\texttt{PPN << 12}作为第一级页表的物理地址；
    \item 以\texttt{VPN0}作为索引值，在第一级页表中找到对应的物理地址页表项，获得物理地址的PPN；
    \item 物理页号对应的物理页基址加上虚拟地址的页内偏移（\texttt{PPN << 12 + vaddr.offset}）就是虚拟地址\texttt{vaddr}对应的物理地址\texttt{paddr}。
\end{itemize}

上述的过程中，CPU利用多级页表对虚拟地址进行逐级翻译，得到其对应的物理地址，这个过程称作 page walk。图中的页表查找过程可以用C语言伪代码表示为下面的伪代码。如之所示，查找过程中，硬件会判断每一级对应的页表是否存在，若存在则继续地址翻译，否则触发缺页异常，等待操作系统处理。

\begin{lstlisting}[language=c]
PTE* pgdir[512]; // 假设这是最高一级的页目录
if (pgdir[vpn2] != 0) { //假如对应的页表项不是空的
    // 从页目录项中获取下一级页目录的虚地址
    // 这里注意，页目录项中存的是物理地址
    // 但CPU都是通过虚地址访问的，因此这里需要转换一下
    PTE* second_level_pgdir = get_va(pgdir[vpn2]);
    if (second_level_pgdir[vpn1] != 0) {
        PTE* last_level_pgdir = 
            get_va(second_level_pgdir[vpn1]);
        if (last_level_pgdir[vpn0] != 0) {
            uint64_t ppn = 
                get_ppn(last_level_pgdir[vpn0]);
            // 这里查找到的ppn就是物理页号
            // 加上页内偏移就可以形成物理地址
        }
    }
}
\end{lstlisting}


当然，S-core中并不要求大家完成页表的动态分配。为了简化、拆分任务，同学们可以静态填好所有页表项。

\begin{note}
    在介绍完页表结构后，我们在这里对“多级页表可以节省空间”解释一二。
    
    以32位操作系统为例，若每个页表项4B，每页4KB。
    如果使用一级页表机制，那么管理全部32位地址空间需要$4\mathrm{GB} \div 4\mathrm{KB} \times 4\mathrm{B} = 4\mathrm{MB}$大小的空间来存放页表。
    而如果使用二级页表机制，需要一个大小为4KB的1024项页目录，负责索引\texttt{[31:22]}这高10位地址，其中每一项又对应一个大小为4KB的1024项二级页表，负责索引\texttt{[21:12]}这中间10位地址，一共需要$4\mathrm{KB} +  1024 \times 4\mathrm{KB} = 4\mathrm{MB} + 4\mathrm{KB}$，比使用一级页表机制还要大。

    但是，进程并不会使用虚拟地址空间中所有的页，没有必要为分配全部页表来进行管理。
    因而在二级页表翻译机制实际的处理方法中，操作系统一开始只会为进程创建第二级的页目录，第一级的页表待有需求时才分配。
    对于程序没有使用的虚地址空间，系统不会浪费内存去记录它，因此多级页表翻译机制能够节约内存空间。
\end{note}



\subsubsection{大页的概念}\label{sec:large_page}

根据上述的介绍，如果页表项的V位为1，且R/W/X位不全为0，则该页是叶子节点。
这样的情况可以发生在多级页表的任何一级页表上。在本任务书中，我们暂时将“利用页表项/\texttt{satp}寄存器找到下一级页表的物理地址、再使用\texttt{VPN}找到虚拟地址在下一级页表中对应的页表项”的过程，也就是 page walk 中的一步，称为一次页表索引。Sv39中：

\begin{enumerate}
    \item 如果索引一步就遇到叶子节点，则计算机认为VPN只有9位，offset有30位，会找到一个大小为$2^{30}\mathrm{B} = 1\mathrm{GB}$的页；
    \item 如果索引两步就遇到叶子节点，则认为VPN有18位，offset有21位，对应找到一个大小为$2^{21}\mathrm{B} = 2\mathrm{MB}$的页；
    \item 如果索引三步才遇到叶子节点，则和我们一般的认知相同，认为VPN有27位，offset有12位，对应4KB的页大小。
\end{enumerate}

所以，如果我们设置好第二级页表的页表项，使之R/W/X位不全为0，那么页表项中44位PPN的前35位（图\ref{fig:sv39pte}中的\texttt{\{PPN[2], PPN[1]\}}）与offset（图\ref{fig:sv39va}的\texttt{\{VPN[0], page offset\}}）组合为最终翻译的物理地址，那我们就成功使用大页进行了地址翻译。
对于1GB和2MB的大页映射，所映射的物理大页也需要是1GB或者是2MB对齐。

大页的优点在于节省页表数量。当虚拟地址空间的大块连续区域的访问权限相同、使用程度均匀时，可以直接使用大页管理，减小页表项数目以降低内存占用度与地址翻译时间开销。大页的缺点在于管理粒度粗、对内存管理程序要求高。操作系统不断为进程分配细粒度空间会导致内存碎片化，若内存管理不佳，寻找一个空闲的大页可能不是那么容易。

\subsection{快表（TLB）及TLB相关指令}

MMU中的快表（TLB, Translation Lookaside Buffer） 作为VPN到PPN的映射的暂存单元，试图挖掘访存局部性，减少地址转换中访存操作次数，大大提高虚实地址翻译效率。

在正常访问过程中，TLB是由硬件自动填充、替换的，操作系统无需操作其中内容。但在TLB中的内容失效、需要清空时，本实验RISC-V指令集的操作系统需要利用 \texttt{sfence.vma} 指令清空整个TLB。
详细来讲，由于不同进程对应的内存地址空间不同，进程切换时，操作系统需要同时切换\texttt{satp}以实现页目录和页表的切换，同时快表内暂存的映射失效。
操作系统主动修改某个页表的页表项后，由于虚实地址转换的映射变动，TLB也会失效。
但TLB并不会自动刷新清空，因为CPU并不知道地址翻译发生变化，因此内核要在修改 \texttt{satp} 后立即使用 \texttt{sfence.vma} 指令告知CPU清空TLB中所有项。

\texttt{sfence.vma} 指令可以以一个虚拟地址作为指令操作数，以刷新指定虚拟地址在TLB中对应的页表项，否则默认刷新整个TLB。
我们已经在框架中将次指令加入设置\texttt{satp}寄存器的函数中，并且在 \texttt{arch/riscv/include/pgtable.h} 头文件中封装好了基于该指令的 \texttt{local\_flush\_tlb\_all()} 与 \texttt{local\_flush\_tlb\_page()} 函数，可供同学们使用。
另外需要注意，如果对地址映射进行了修改，必要的时候还需要用 \texttt{fence} 和 \texttt{fence.i} 刷新数据/指令cache，后者被封装成了 \texttt{local\_flush\_icache\_all()} 函数。


\subsection{虚拟内存的软硬件协同}

读到这里，不知道同学们对实现虚拟内存机制需要执行的任务是否已经清楚，如果仍觉得有不清楚之处，希望大家能仔细阅读任务书及其他资料，在理解概念、完成设计之后再开始编程。在这一节中，我们也将分别整理软件和硬件的任务，方便大家的理解。

\subsubsection{硬件的工作}

虚拟内存管理中硬件的工作总结如下：

\begin{itemize}
  \item 判断虚拟内存是否已被启用。若是，则尝试查询TLB，寻找是否已有缓存好的虚实地址映射。
  \item 若TLB不能协助翻译，则进行虚拟地址到物理地址的转换。硬件会自动根据\texttt{satp}寄存器与当前的虚拟地址，查询多级页表，根据当前节点是否为叶子节点决定是应当将PPN和offset拼接成为最终的物理地址，还是继续检索下一级页表。
  \item 进行权限检查，根据页表项中\texttt{X/W/R}等位进行权限判断，如果权限错误则触发对应例外。
  \item 检查虚拟地址是否存在对应的映射即是否缺页，如果缺页在则触发对应的例外。
\end{itemize}


\textbf{上述提到的例外 在Project 2 任务书中的例外表有相关的说明，请大家注意查看。}

\subsubsection{软件的工作}

虚拟内存管理中软件的工作（也就是操作系统的工作）总结如下：

\begin{itemize}
  \item 通过管理相应的CSR控制当前的访存模式。
  \item 操作系统需要为进程建立需要的页目录，并将根页表的PPN填入\texttt{satp}寄存器中，在进程切换时也需要对应的切换\texttt{satp}中的PPN。
  \item 处理与虚实地址转换、访存相关的一些例外。
  \item 在有需要时刷新TLB。
\end{itemize}

同学们到这里应当基本了解虚拟内存机制的实现方式了。我们下面将介绍各个任务，带领同学们为内核、用户程序启用虚拟内存机制，再来逐步完善它。
  
\subsection{任务一：启用虚拟内存机制并进入内核}

\subsubsection{内核虚拟地址空间介绍}

为了能使得我们的内核能够启用虚拟内存，框架代码在编译、链接时就需要将内核安排在虚拟地址空间上，即将内核的入口设置为内核虚拟地址。
启用 SV39 分页模式后，39位虚地址最高位为1意味着该空间为内核空间，否则为用户空间，即用户与内核平分共512GB的虚拟内存地址空间。本实验中我们严格遵循这个规范。

在内核页表初始化时，我们直接将物理地址空间与前32位为\texttt{0xffffffc0}的虚地址空间（最高位\texttt{[38]}为1/\texttt{[37:32]}这6位为0的Sv39虚拟地址，后32位任意选取）之间建立一一映射，开启虚拟内存后，内核便在这个虚地址空间上运行，在形式上这比起运行在物理地址空间只是添加了一个地址前缀。我们在\texttt{Makefile}中将内核入口地址设置为\texttt{0xffffffc0\_50202000}，它实际对应着物理地址\texttt{0x50202000}。为了方便内核管理物理内存，内核使用的虚实地址转换映射一般为平移操作。因此，我们的操作系统内核能够自由支配4MB大小的物理内存。（因虚拟地址过长，为方便读者阅读，任务书中将64位虚拟地址以下划线分成两部分表示）

\begin{note}
    本实验中，物理地址\texttt{0x50200000}会被映射到\texttt{0xffffffc0\_50200000}，而 \texttt{0x55200000} 会被映射到\texttt{0xffffffc0\_55200000}，内核使用的虚拟地址是物理地址平移\texttt{0xffffffc0\_00000000}的结果。
    这样，如果我们想在内核中管理多级页表，那么根据页目录记载的下一级页表物理地址直接做平移（加上固定的偏移量），就能得到内核可以使用的虚地址，我们就可以通过这个内核虚拟地址访问下一级页表。
    如果我们想要为进程分配一个4KB的用户态物理地址页面，比如\texttt{0x55201000}，只需在内核中分配地址\texttt{0xffffffc0\_55201000}处的页面。
    通过以内核虚拟地址表达物理地址的方式，内核可以轻松地管理整个物理内存。

    一旦启用虚拟内存，指令便只能通过虚拟地址进行访存，无法直接使用物理地址，所以很多情况下必须使用物理地址对应的内核虚拟地址。
    鉴于不同进程的内核虚拟地址映射相同，在靠后的实验当中，同学们可以考虑使用相同的页表完成不同进程在内核态中的地址翻译。
\end{note}

\subsubsection{启用虚拟内存机制并进入内核}

在CPU刚刚启动、进行初始化时，虚拟内存机制并没有立即启用，也没有页表可供地址翻译使用，操作系统直接运行在物理地址空间上。
在初次进入操作系统内核的初始化过程中，内核需要一段使用物理地址的代码来准备内核页表、启用虚拟内存。

我们将内核的入口指定为这一段代码，当boot block搬运内核到对应的物理地址后立即跳转到内核入口开始执行这一段代码。
这一段代码我们已经在 start code 框架中提供，核心代码位于 \texttt{arch/riscv/kernel/start.S} 和 \texttt{arch/riscv/kernel/boot.c}中。

\begin{note}
    由于Makefile与链接器的设置，这段代码和内核被编译到一个输出文件中，且一并按照启用内核虚拟地址后进行链接，所以这段代码如若使用标号作为绝对地址，链接器会认为它需要一个内核虚拟地址，而非物理地址，因而出现地址计算错误。
    而如果使用相对地址寻址，由于我们选用平移作为内核虚拟地址的地址转换映射，链接器求出的两个内核虚拟地址的差，恰好等于实际需要计算的两个物理地址的差，故不会出现错误。

    所以说，这段代码内所有的分支跳转、数据寻址都不能使用绝对地址，只能使用相对于当前PC的相对地址。
    这对代码的编写提出了严格的要求，比如不能使用 \texttt{switch} 语句、不能使用指针数组等等。
\end{note}

框架应已在编译时将内核入口指定为 \texttt{\_boot}，这段代码位于 \texttt{arch/riscv/kernel/ start.S} 中。

% \begin{enumerate}
% \item 单独编译一份在实地址上运行的程序A，在bootblock时其将会被首先搬运到内存中。将内核作为一个特殊的APP，由A完成内核页表的建立并打开虚拟内存后将真正的内核从SD卡中加载到内存中。
% \item 由内核自己完成内核页表的建立和打开虚拟内存，并跳转到内核虚拟地址上。
% \end{enumerate}

\texttt{arch/riscv/kernel/boot.c} 中的\texttt{boot\_kernel()}函数将会建立内核页表，把内核空间映射为2MB的大页跳转到内核的真正入口。核心代码如下：

\begin{lstlisting}[language=c]
extern uintptr_t _start[];
/*********** start here **************/
int boot_kernel(unsigned long mhartid)
{
  if (mhartid == 0) {
    setup_vm();
  } else {
    enable_vm();
  }

  /* 进入内核后将永远不会返回 */
  ((kernel_entry_t)pa2kva(_start))(mhartid);

  return 0;
}
\end{lstlisting}

\texttt{boot\_kernel()} 的参数为\texttt{mhartid}。
启用虚拟内存机制后，需要跳转到标号\texttt{\_start}（即之前实验中的内核入口），但此处跳转指令中的\texttt{\_start}地址是相对\texttt{PC}计算所得，是\texttt{\_start}的内核虚拟地址与当前指令的内核虚拟地址的差；而我们刚开启虚拟内存，尚在使用物理地址（其实是和物理地址相等的虚拟地址，此时内核采用临时的映射将其映射到相等的物理地址上）执行程序，以物理地址计的\texttt{PC}并不能依靠相对地址计算出\texttt{\_start}的内核虚拟地址，理论上我们需要将\texttt{PC}转化为对应的内核虚拟地址，才能正确使用相对地址跳转。但是如此改变\texttt{PC}并不现实，所以我们将得到的\texttt{\_start}的物理地址转换成内核虚拟地址，然后再进行跳转。这就是\texttt{((kernel\_entry\_t)pa2kva(\_start))(mhartid)}这段代码的由来，同学们可以使用gdb查看程序动态执行过程，并结合编译产生的二进制代码，理解此处程序的设计。

如果同学们想要使用双核进行实验，那么需要注意，主核和从核各自是一个CPU，各有一套CSR，因此各有一个\texttt{satp}寄存器。主核需要完成内核页表的映射并设置\texttt{satp}寄存器、启用虚拟内存，从核只需要设置\texttt{satp}寄存器。


目前整个内核的启动流程变成了 \texttt{\_boot->boot\_kernel->\_start->main}。前两个阶段运行在实地址上，只访问物理地址。随后都运行在内核虚拟地址上，只访问虚地址。

同学们可以把实验一分为两部分完成。start code 中已经完成了启用虚拟内存机制的框架代码，第一部分中同学们需要补全、修改部分代码，启动内核并打印出框架中已经写好的提示信息。第二部分中，同学们需要完成“任务一续”的内容，实现用户程序的加载和运行。我们已在\texttt{main()}函数中添加下面的代码来拆分这两个部分：

\begin{lstlisting}[language=c]
/*
 * Just start kernel with VM and print this string
 * in the first part of task 1 of project 4.
 * NOTE: if you use SMP, then every CPU core should call
 *  `kernel_brake()` to stop executing!
 */
printk("> [INIT] CPU #%u has entered kernel with VM!\n",
    (unsigned int)get_current_cpu_id());
// TODO: [p4-task1 cont.] remove the brake and continue to start user processes.
kernel_brake();
\end{lstlisting}

这段代码的作用是让CPU在完成内核的部分初始化工作后打印一行提示：\texttt{"CPU \#$i$ has entered kernel with VM!"}（其中$i$是当前CPU核的ID），然后调用函数\texttt{kernel\_brake()}暂停（该函数将关中断然后陷入无限循环），而不再执行后面的程序。同学们若能开启虚拟内存机制启动内核并在屏幕上看到这行输出（实现C-core的同学为启动双核，应见到两行输出，并且可能需要考虑屏幕光标的位置，以防止两个核的打印结果重叠），则可认为任务一的第一部分完成。在接下来的“任务一续”中，请同学们移除对\texttt{kernel\_brake()}的调用，以正常执行后续操作。

\subsubsection{实验要求}

启用虚拟内存机制并进入内核。

\subsubsection{实验步骤}

\begin{enumerate}
    \item 请大家仔细阅读注意事项后再开始本次任务。
    \item 修改 \texttt{arch/riscv/boot/bootblock.S}，将内核搬运到\texttt{0x50202000}处。
    \item 内核页表映射的代码我们已经为大家写好，请大家认真阅读并理解代码实现。熟悉这一部分的代码对后续建立用户页表有所帮助。
    \item 完成 \texttt{arch/riscv/include/pgtable.h} 中的函数，以供本实验中使用，其功能如表\ref{tab:pgtable_function}所示。
\end{enumerate}

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[htbp]
  \centering
  \caption{\texttt{pgtable.h}中提供的函数功能}
    \begin{tabular}{ll}
    \toprule
    \textbf{函数名称} & \textbf{函数功能} \\
    \midrule
    \texttt{kva2pa(uintptr\_t kva)} & 将内核虚地址转换为物理地址 \\
    \texttt{pa2kva(uintptr\_t pa)} & 将物理地址转换为内核虚地址 \\
    \texttt{get\_pa(PTE entry)} & 从页表项读取物理地址 \\
    \texttt{get\_pfn(PTE entry)} & 从页目录读取页框号 \\
    \texttt{set\_pfn(PTE *entry, uint64\_t pfn)} & 为页表项设置页框号 \\
    \texttt{get\_attribute(PTE entry, uint64\_t mask)} & 从页表项读取页属性 \\
    \texttt{set\_attribute(PTE *entry, uint64\_t pfnbits)} & 为页表项设置页属性 \\
    \texttt{clear\_pgdir(uintptr\_t pgdir\_addr)} & 清空页目录 \\
    \bottomrule
    \end{tabular}%
  \label{tab:pgtable_function}%
\end{table}%

\subsubsection{注意事项}

\begin{enumerate}
    % 24-sprint P4 final special
    % \item 为了降低大家调试的难度，推荐大家暂时将之前实现的网卡驱动以及文件系统相关的部分注释，感兴趣的同学可以在任务五中进一步适配上相关的功能。
    
    \item \texttt{boot\_kernel()} 的运行同样也需要内存上的一段物理地址空间，作为其栈空间使用。start code 将 \texttt{0x52001000-0x52002000} 这一段地址作此用处，同学们注意不要在别的地方使用这一段内存。双核启动可能需要两段栈空间，分别给两个不同的核使用，完成C-core的同学请自行设计这段栈空间的分配。
    
    \item 因为内核编译时已经按照内核虚拟地址编译，所以导入内核的符号表中，指令对应的地址也是内核虚拟地址。
    可以在 \texttt{make gdb} 后在gdb中输入命令 \texttt{add-symbol-file build/main 0x50202000}，动态加载内核的符号表到地址 \texttt{0x50202000}，以使用gdb调试内核页表映射部分的实地址代码。
    
    \item 建立完物理地址到内核虚拟地址的映射以后，就可以开启虚拟内存机制，然后加载内核了。
    因为 \texttt{boot\_kernel()} 部分的代码是运行在物理地址上的，开启虚拟地址后，\texttt{boot\_kernel()}的代码就也会通过虚拟内存机制来访问内存。
    为保证其的正确性，需要临时把 \texttt{0x50200000-0x51000000} 这段虚拟空间映射到相同的物理空间上。
    Start code 已在\texttt{setup\_vm()}函数中设置好这一临时映射，进入内核的\texttt{main.c}函数之后，同学们需要\textbf{取消这一临时映射}，避免日后与用户态虚拟地址空间产生冲突。C-core的同学因为要启动双核，所以需要考虑好取消临时映射的时机。

    % 内核的页表是在内核加载前就需要设置好的，主要代码位于start.S和boot.c中，这部分的代码我们在start-code中已经给出，
    % 这里需要同学们阅读建立内核页表的相关代码并理解其含义，在理解的基础上完成建立页表过程中使用的功能函数的相关代码。
    % 由于我们的内核是提前拷贝完成的，目前不需要动态分配页面，这里我们采用两级页表，此时每个页面的大小为2MB。
    % 在内核页表初始化时，我们直接将物理地址空间与带有0xffffffc0前缀的虚地址空间（即内核虚拟地址空间）进行一一映射，开启虚拟内存后，内核就运行在这个虚地址空间中，
    % 这是为了平分Sv39页表能够映射的地址空间（最高位为0代表其为用户空间，为1代表其为内核空间，最高位扩展到64位），与运行在物理地址空间时相比，只是添加了一个地址前缀。
    % 如果在虚地址模式下需要对特定的物理地址进行操作，也可以利用这个一一映射的特性。欢迎同学们通过查阅资料了解更多有关Sv39页表机制的知识。

    % 整体结构上，之所以像代码中这么实现的原因是，虚拟内存一旦开启，则所有访问都会被认为是虚地址，
    % 因此，我们需要首先用一小段程序建立内核页表，并将内核拷贝到对应的虚地址上。
    % 之后开启虚拟内存，再跳入到对应的位置上去。原先我们是bootloader->kernel，
    % 在本次实验中，变为了bootloader->boot\_kernel->kernel。

    %初始页表的位置推荐设置在PGDIR\_PA宏定义的那个位置上，当然，换成别的也可以。

    % 目前，整个内核的结构变成了：由start.S和boot.c链接在一起，形成start的部分，由该部分开启虚拟内存并加载内核。
    % head.s、main.c等部分会编译链接在一起，形成真正的内核部分。

    %这里有一个小技巧：为了内核管理物理内存方便，一般内核虚拟地址和物理地址之间都是线性映射的。
    %例如假设物理地址是0x50200000，它会被直接映射到0xffffffc0\_5020000。
    %如果物理地址是0x55200000，它会被直接映射到0xffffffc0\_5520000。
    %这样做是为了管理页面方便，比如我们如果想在内核中访问某个进程的页表，我们先根据页目录，找到二级页表的物理地址，
    %然后直接加上这个固定的偏移量，就能得到相应的内核虚拟地址。
    %通过这个内核虚拟地址，我们就可以访问到下一级的页表
    %（注意，我们的程序访问东西只能通过虚地址进行，没法直接用物理地址访问，所以我们必须知道页表的物理地址对应的内核虚拟地址）。

    % 建立完物理地址到内核虚拟地址的映射以后，就可以开启虚拟内存机制，然后加载内核了。
    % 这里需要注意的是，因为start部分是运行在物理地址上的，开启了虚地址以后，start的代码就也会通过虚拟内存机制来访问内存。
    % 为了让start的代码不出问题（start的代码都是在0x50201000这一段运行的），所以需要临时把0x50201000所在的2MB的页映射到0x50201000这个虚地址上，
    % 这样才能让start的代码在虚拟内存开启的情况下也能正确运行。
    % 这一映射方式已经由start-code提供好，在进入内核后，同学们需要将这一临时映射取消掉，避免后面用户程序用到这部分虚拟地址空间与内核地址冲突。

    \item 在实验环境配备的开发板上，\texttt{bios\_sdread()} 函数无法一次读取过多sector，建议同学们编写一个函数对其包装，在此函数中使用循环结构，按照至多64个sector一组分多次读入。

    \item 内核进入\texttt{\_start}并完成.bss段的清空后，便可继续开始下面的任务。
\end{enumerate}

\subsection{地址空间管理}

在Project 2/3中，我们在开发版和QEMU上利用DASICS功能来实现对实地址空间中用户地址空间与内核地址空间的隔离。在打开虚拟内存之后内核与用户空间，以及进程之间都有了比较好的内存隔离。因此在本实验中，我们抛弃DASICS检查，不需要再使用\texttt{loadbootd}启动内核，而是使用\texttt{loadboot}和\texttt{loadbootm}分别启动单核和双核。

在Project 4及其之后的实验中，\textbf{内核将管理所有的物理地址}。前一节中我们已经提到内核虚拟地址和物理地址是线性映射的关系，因此在本实验中我们以内核的视角来管理和分配地址空间。建议的地址空间划分如所表\ref{tab:address-usage}所示。

\begin{table}[hbtp]
    \centering
    \begin{tabular}{ll}
        \toprule
        地址范围                  & 建议用途            \\
        \midrule
        \texttt{0xffffffc0\_50000000 - 0xffffffc0\_50200000} & BBL代码及其运行所需的内存  \\
        \texttt{0xffffffc0\_50200000 - 0xffffffc0\_51000000} & Kernel的数据段/代码段等 \\
        \texttt{0xffffffc0\_51000000 - 0xffffffc0\_52000000} & Kernel页表以及跳转表等 \\
        \texttt{0xffffffc0\_52000000 - 0xffffffc0\_60000000} & 供内核动态分配使用的内核虚拟地址空间    \\
        \bottomrule
    \end{tabular}
    \caption{内核地址空间用途划分}
    \label{tab:address-usage}
\end{table}




\section{用户进程}

在之前的实验中，我们所使用的用户进程在本质上更类似于线程，因为这些进程之间的地址空间并没有真正隔离开，而是共享了特定的资源和运行空间。
在执行过程中，每个进程的地址空间都是事先约定、分配好的，无法拥有独立的运行环境。
然而，在这次实验中，我们需要通过虚拟内存管理机制，提供独立的进程运行环境。
具体来说，每个应用程序将拥有相同的入口地址，不再需要依赖严格的地址空间范围约束，并能够在操作系统的管理下独立加载和执行。
这种设计不仅为每个应用分配了独立的虚拟地址空间，还显著增强了不同进程之间的隔离性，从而降低了相互干扰的风险。
此外，这种改进的进程隔离机制能够帮助操作系统更高效地分配和管理资源，为系统的安全性和稳定性提供了进一步保障。

\subsection{shell}

本次实验中，同学们需要继续使用之前设计的shell用户程序，并在虚拟内存机制启用后，继续支持其中功能。
本次任务需要支持\texttt{exec}、\texttt{kill}、\texttt{ps}、\texttt{clear}等四个命令，要求与 Project 3 中相同。
推荐同学们添加一个命令，可以显示出所有可以exec的用户程序名与需传递的参数。

\subsection{加载用户程序}\label{sec:load_app}

本次实验中，BIOS提供了SD卡读写的相关函数，\textbf{传入的地址参数均应使用物理地址。}
% 另外bios\_putstr函数\textbf{传递的str指针参数也需要转换为物理地址地址}，
% 我们已经在补丁中将include/os/kernel.h中相关的bios接口函数的地址使用kva2pa函数（这个函数大家在任务一启用虚拟内存机制进入内核时就已经完成）进行进行转换，请大家注意查看。
\textbf{在进入到内核的\texttt{\_start}后，所有地址访问都使用虚地址，内核当中不要出现任何的直接对物理地址的访问。}
% 另外需要提及的是，目前QEMU上的从核读读写SD卡的操作仍然
% 存在问题，我们建议做双核的同学在QEMU上先关闭一个核测试功能，功能正确后再打开双核进行上板测试。

\subsubsection{为用户程序建立页表映射并拷贝数据到映射的物理页面}

% 本次实验中用户程序的加载方式与之前一致，但是需要修改以前的loader，要对目标虚地址（即entrypoint）建立页表并分配物理页之后才能够进行拷贝。
% 这里需要特别注意的是，对于BIOS中SD卡操作的相关函数，需要传入的地址参数为物理地址，而非内核虚拟地址。
% 同时，如之前所提到的，在开始实现应用加载功能时，需要实现用户页表的相关功能。
% 在开始实现该功能时，可以先做静态分配，加载应用时即分配足够多的页面（如二级页表的2MB），以保证程序不需要后续分配页面也能正常运行，这一步可以参考我们建立内核页表的过程进行实现。

在开启虚拟内存后，加载用户程序变得不再是简单地将用户程序从SD卡读入到内存中的指定地址并直接执行。
我们需要在加载用户程序时建立好相应的页表体系（虚拟地址到物理地址的映射），以确保在切换到用户进程后，它能够在对应的虚拟地址空间中正常运行。
在前面的实验中，大家已经了解过，内核在启动后首先加载用户进程shell，随后可以通过\texttt{sys\_exec}系统调用在shell中启动新的用户测试程序。
这两种加载方式有一个共同点：在当前进程的地址空间中加载另一个进程。
可以这样理解，子进程的地址空间映射是由父进程来完成的，因此这是一个跨地址空间的操作。
针对三级页表，为子进程的某个虚拟页面分配好物理页面并建立页表映射后，还需要将用户程序的数据载入对应的虚拟地址空间，也就是将程序内容拷贝到映射的物理页面中。
因为最终CPU在访问内存时使用的仍然是物理地址（虚拟地址翻译成的物理地址），接下来我们将详细说明如何实现这个过程。

start code中的 \texttt{include/os/mm.h} 头文件中中定义了一个名为 \texttt{alloc\_page\_helper()} 的函数，能够为指定的虚拟地址建立页表映射，他的定义如下：

\begin{lstlisting}[language=c]
// va 为需要映射的虚拟地址，pgdir为页表目录，
// 返回值为为va映射的物理地址对应的内核虚拟地址
uintptr_t alloc_page_helper(uintptr_t va, uintptr_t pgdir);
\end{lstlisting}

\texttt{alloc\_page\_helper()}为指定的虚拟地址\texttt{va}建立好页表映射之后，返回为\texttt{va}映射的物理页框对应的内核虚拟地址，我们只需要将SD卡中的内容加载到该内核虚拟地址上即可。

\begin{note}
    本实验中，大家被要求以直接制造一个新进程的方法实现exec系统调用，然后在shell中使用此系统调用，进行新进程的创建。但在Linux操作系统中，新进程的创建则有所不同，是父进程使用 \texttt{fork()}创建子进程，子进程再使用\texttt{exec()}加载新的程序。
    本实验中大家将实现虚拟内存管理功能，若同学们有兴趣，可以更改本实验中shell的exec功能，令其执行系统调用\texttt{fork}创建子进程，并在子进程中使用\texttt{exec}系统调用执行新的程序，并在执行完成之后使用\texttt{exit}系统调用自行退出。
\end{note}


\subsubsection{用户程序页表映射细节}\label{sec:usr_page_table_mapping}

在Project 1中，大家已经实现了将测试程序的ELF文件制作成镜像的\texttt{createimage.c}程序。在这里，我们将进一步探讨ELF文件的结构，特别是其程序头表。首先，以下是64位ELF文件的程序头表数据结构：

\begin{lstlisting}[language=c]
/* Program segment header.  */

typedef struct
{
  Elf64_Word    p_type;                 /* Segment type */
  Elf64_Word    p_flags;                /* Segment flags */
  Elf64_Off     p_offset;               /* Segment file offset */
  Elf64_Addr    p_vaddr;                /* Segment virtual address */
  Elf64_Addr    p_paddr;                /* Segment physical address */
  Elf64_Xword   p_filesz;               /* Segment size in file */
  Elf64_Xword   p_memsz;                /* Segment size in memory */
  Elf64_Xword   p_align;                /* Segment alignment */
} Elf64_Phdr;
\end{lstlisting}

在程序加载时，操作系统会通过ELF文件的程序头表信息来解析和映射段的虚拟地址空间。通过\texttt{p\_type}字段，操作系统可以区分不同类型的段（如加载段、动态段等），而\texttt{p\_vaddr}则标明了该段在虚拟地址空间中的位置。此外，\texttt{p\_offset}和\texttt{p\_filesz}字段提供了段在文件中的位置和大小，使得系统可以将其正确加载到内存。

我们使用Linux的\texttt{readelf}工具获取一个简单的测试程序的程序头表信息，得到如下结果：

\begin{lstlisting}
  Elf file type is EXEC (Executable file)
  Entry point 0x10000
  There are 2 program headers, starting at offset 64
  
  Program Headers:
    Type           Offset             VirtAddr           PhysAddr
                   FileSiz            MemSiz              Flags  Align
    LOAD           0x0000000000001000 0x0000000000010000 0x0000000000010000
                   0x0000000000000c35 0x0000000000001c40  RWE    0x1000
    GNU_STACK      0x0000000000000000 0x0000000000000000 0x0000000000000000
                   0x0000000000000000 0x0000000000000000  RW     0x10
  
   Section to Segment mapping:
    Segment Sections...
     00     .text .rodata .sdata .sbss .bss 
     01     
\end{lstlisting}

该结果中程序头表的 \texttt{Type} 到 \texttt{Align} 字段分别对应\texttt{ELF64\_Phdr}结构体中的\texttt{p\_type}到\texttt{p\_align}字段。在加载可执行文件时，我们只需要关心\texttt{p\_type}为\texttt{LOAD}类型的字段。为了实验环境的简单，我们的实验框架尽量让所有ELF只会存在一个\texttt{LOAD}类型的程序段，如果大家见到了两个，请当心是否有问题。ELF头中部分成员的详细含义如表\ref{tab:phdr_table}所示。

\begin{table}[hbtp!]
  \begin{center}
      \begin{tabular}{|c|l|}
          \hline
          成员 & 含义 \\
          \hline  
          \texttt{p\_vaddr}  & \begin{tabular}[c]{@{}l@{}}
                      “Segment”第一个字节在进程虚拟地址空间的起始地址，在我们\\的实验中，\textbf{这个虚拟地址等于进程的入口。} \\
                      \end{tabular}\\
          \hline
          \texttt{p\_filesz} & \begin{tabular}[c]{@{}l@{}}
                      “Segment”在ELF文件中所占空间的长度，可能为0，因为有可\\能这个“Segment”在ELF文件中不存在内容。 \\
                      \end{tabular}\\
          \hline
          \texttt{p\_memsz}  & “Segment”在进程虚地址空间中所占的长度，它的值也可能为0。 \\
          \hline
          \texttt{p\_flags}  & “Segment”的权限属性，比如可读“R”，可写“W”和可执行“X”。 \\
          \hline
      \end{tabular}
  \end{center}
  \caption{程序头表成员及其含义}
  \label{tab:phdr_table}
\end{table}

在程序头表中，\texttt{p\_filesz}字段表示当前段（segment）在ELF文件中所占的存储大小，而\texttt{p\_memsz}字段则表示该段加载到内存后所需的空间大小。一个段通常由多个节（section）组成，对于其中数据全为0的.bss段，没必要浪费文件存储空间来保存一段全为0的数据。
ELF文件的设计者采用了一个巧妙的方式：令\texttt{p\_memsz}大于或等于\texttt{p\_filesz}，操作系统在加载ELF文件时，自动向ELF文件无法覆盖到的部分填充0。这种设计不仅有效地节省了文件的存储空间，还满足了.bss段在内存中初始数据为0的假设。

因此，本实验中，建立用户态的页目录、页表体系，就是准备好自程序入口起\texttt{p\_memsz}大小的空间的地址转换映射。
以上方用以查看程序头表的测试程序为例，我们应当为应用程序映射 \texttt{0x10000-0x11c40} 的虚拟地址空间（两个4KB的页）。
这里需要将SD卡中长度为\texttt{0xc35}的数据拷贝到为虚拟地址 \texttt{0x10000-0x10c35} 映射的物理页面中时，如果我们为之映射了 \texttt{0x52000000-0x52000c35} 的物理地址空间，则需要将SD卡中的数据拷贝到 \texttt{0xffffffc0\_52000000-0xffffffc0\_52000c35} 的内核虚拟地址空间。
同时将为\texttt{0x10c35-0x11c40}映射的物理地址空间填充0，以满足.bss段起始数据为0的假设。这也可以在\texttt{crt0.S}中进入到用户程序的\texttt{main()}函数之前完成。

\subsubsection{初始化进程页表项标志位设置}

首先，对于非叶子节点的页表项，需要将V位置位，代表页表项有效，\textbf{其余的标志位清零}。

对于叶子节点的页表项：

\begin{itemize}
    \item \texttt{V}：置位，代表该页表项有效。
    \item \texttt{R/W/X}： 程序头表中\texttt{p\_flags}字段则对应着页表叶子节点页表项的\texttt{R/W/X}位。在我们的实验中，\texttt{p\_type}为\texttt{LOAD}类型字段的权限都为\texttt{RWE}，因此在为用户程序建立页表时，需要将末级页表项的\texttt{R/W/X}位全部置位。
    \item \texttt{U}：对需要在用户态使用的页，置位，使之能够访问成功。
    \item \texttt{G}：无需关心。
    \item \texttt{A/D}：在QEMU上，当访问该页面时，会根据访问或者写入自动置位；在我们的板子上，如果 \texttt{A} 为 0 且发生了对这个页面的访问，或者 \texttt{D} 为 0 且发生了对这个页面的写入，都会直接触发缺页异常。因此我们建议大家在完成任务一和任务二时将这两位置一。A/C-core在后续的换页机制中，如果大家设计的换页算法比较好，是需要根据这两位来选择换出的页面的，这时请大家初始化时就不要设置这两位，等到发生了相应的例外再处理。S-core的同学在初始化时请将这两位置为1。
\end{itemize}


\subsection{任务一续：执行用户程序}

\subsubsection{实验要求}

加载shell作为第一个进程启动，支持\texttt{exec}、\texttt{kill}、\texttt{ps}、\texttt{clear}命令。可以执行起\texttt{fly}程序，显示小飞机。

\subsubsection{实验步骤}

\begin{enumerate}
    \item 请大家仔细阅读注意事项后再开始本次任务。 

    \item 开启虚拟内存后，内核中的所有地址访问都必须通过虚拟地址进行。
    请将之前直接使用物理地址的代码部分进行修改，确保所有的内核访问都通过映射后的内核虚拟地址完成。

    \item 修改内存管理模块。

    \item 需要完成A/C-core的同学若之前未曾添加，请为\texttt{task\_info\_t}新增成员存储程序头表的\texttt{p\_memsz}字段。

    \item 为每个新进程分配一个4KB的页面作为用户页表目录，拷贝内核页表到此页面。建议大家将用户进程的页表对应的内核虚拟地址存储到PCB结构体中以方便管理。若同学们对实现内核虚拟地址的翻译有其他想法，也可以尝试自主实现。

    \item 认真阅读理解\ref{sec:load_app}节的内容，修改loader的实现。将根页表目录作为参数传递给loader，由loader完成用户虚实地址映射并从SD卡中载入用户程序，此处对于S-core和A/C-core的同学要求不同，请注意查看注意事项。

    \item 修改调度器，调度时切换用户页表。

    \item 修改kill和exit等系统调用的实现，回收分配的物理页。
\end{enumerate}

\subsubsection{注意事项}
\begin{enumerate}
    \item 做S-core的同学不需要改动\texttt{Makefile}，做A/C-core的同学需要把\texttt{Makefile}中的\texttt{USER\_ENTRYPOINT}修改为\texttt{0x10000}。
    与Project 3一致，做S-core的通过 \texttt{exec [id]} 的方式启动测试，A/C-core的同学通过
     \texttt{exec name} 启动测试。在本章的测试中，完成A/C-core的同学启动测试时需添加\texttt{\&}选项。
    
    \item 我们已经将内核框架中的\texttt{KERNEL\_JMPTAB\_BASE}和\texttt{BIOS\_FUNC\_ENTRY}宏定义更改为内核虚拟地址，请大家注意查看，其余大家自己设计的使用物理地址的部分，请大家自行更改。
    
    \item 内核中的\texttt{pid0\_pcb}也是需要页表目录的，由于它只需要在内核空间中运行，因此它的页表即为内核页表，对应着 \texttt{arch/riscv/include/pgtable.h}中的宏定义 \texttt{\#define PGDIR\_PA 0x51000000lu}。这是一个物理地址，请将其转化为内核虚拟地址存储到\texttt{pid0\_pcb}中。
    
    \item 本实验中我们提供了简单的内存管理模块代码以供参考，代码实现在 \texttt{kernel/mm/mm.c} 以及头文件 \texttt{include/os/mm.h} 当中，这个简单的内存管理模块中还包含了S-core的大页分配的简单示例。如果大家之前在 \texttt{mm.c} 中自己实现了内存管理算法，这两个文件的冲突可能比较多，请大家对照着start code仓库中的代码按需修改。
    % 考虑到大家之前可能已经实现了比较完善的内存管理算法，因此我们没有打这一部分的补丁，请大家到patch中的startfull文件夹查看相应文件进行修改。


    \item 对于用户页表的建立，本次实验对S-core和A/C-core有不同的要求。
    \begin{itemize}
        \item S-core：为了降低难度，我们要求用户进程只需要建立两级页表体系（即只需要实现2MB的大页），并规定用户内存空间只有2MB，将这片空间的顶部作为用户栈栈顶，具体结构如图\ref{fig:p4-Score-mem}所示。因此对于S-core，只需要将虚地址\texttt{0x200000}映射到分配的2MB大页即可，页表仍需要内核分配4KB的页面。2MB的大页映射可以参考内核页表的映射方法。
    
        由于目前的用户程序体量比较小，2MB的大页存放用户进程绰绰有余，不需要过多关心\ref{sec:usr_page_table_mapping}节中提到的\texttt{p\_memsz}和\texttt{p\_filesz}的细节。将用户程序从SD卡中读取到映射的物理内存中的操作会比较容易。2MB的大页包含了用户栈空间，因此不需要单独映射用户栈。
        
        因为Sv39中页表仍然是占用4KB的页框，因此S-core需要支持分配4KB和2MB的页面，前面我们已经提到对应的页面物理地址也需要是4KB或者是2MB对齐。因此我们建议S-core的同学仿照Project 2/3中的地址划分模式，划分出两块空间，分别用于分配4KB的页面和2MB的页面。
        
        不过，由于\ref{sec:large_page}节中所讨论的不足之处，我们鼓励大家尝试建立完整的三级页表。
        
        \begin{figure}[!htbp]
            \centering
            \includegraphics[width=0.5\textwidth]{p4-Score-mem}
            \caption{S-core用户地址空间图}
            \label{fig:p4-Score-mem}
        \end{figure}
        
        \item A/C-core：我们要求大家建立三级页表（即实现4KB大小的页分配机制），并根据\texttt{task\_info}中存储的信息，映射对应大小的地址空间。
        
        由于应用程序占用的内存空间有可能大于4KB，因此可能需要分配多个页。两个在虚拟地址空间中连续的虚拟页面，映射到的物理页框未必是连续的，所以任何需要使用内核虚地址的连续内存操作都不应该跨页进行，如自SD卡向内存中装载程序。同学们需要建立一个包装函数，在内部利用循环结构，控制使用虚拟地址的内存读写按页大小进行。
        % 这里为了实现的简单，我们建议大家首先将用户程序整个读取到缓冲区中。随后再进行拷贝。
    \end{itemize}


    \item 现在需要设定用户进程的用户栈到固定位置。对S-core而言，栈底地址固定为\texttt{0x400000}；对A/C-core而言，推荐使用\texttt{0xf00010000}作为栈底地址，同学们需要为之准备一个物理页框。注意用户栈的增长方向，栈底地址实际上较高。

    在切换进程时，根页目录和\texttt{ASID}一并需要被切换。因为所有进程的内核虚拟地址映射关系都一样，所以\texttt{satp}寄存器的切换不会影响内核态的执行。将页表的基地址写入到\texttt{satp}后，需要刷新TLB，否则可能出现QEMU通过但FPGA开发板上不通过的现象。

    \item 内核初始化的时候\texttt{sstatus}寄存器的置位需要被关注，目前请确保\texttt{SUM}位已经被置为1。这一位控制着Supervisor态下的程序是否可以访问U位为1的页面。一直对\texttt{SUM}位置位并非是一件合适的事情，出于安全性的考虑，同学们可以了解

    \item 对于A/C-core而言，Project 3中实现的命令行参数的拷贝，在本实验中也会变成跨地址空间的内存访问，需要大家自行设计实现。
    % （提示：可以利用
    % 内核虚拟地址和物理地址是线性映射这一关系）。
\end{enumerate}

\subsubsection{要点解读}

\begin{enumerate}
    \item 在添加虚拟内存机制后，干净地回收一个进程所使用的所有内存成为一个挑战。请大家思考具体如何实现这一件事。

    \item 请务必铭记，CPU发出的所有内存访问请求均是基于虚拟地址。在C语言代码中，所有的访问操作都必须通过虚拟地址进行，而不是直接使用物理地址。

    \item 如果不将内核页表映射到用户页表中，当用户进程陷入内核时，一旦访问内核虚拟地址，必然会发生缺页异常。因此，我们需要为每个用户进程映射“一个”内核页表。

    内核态地址空间页表是在系统启动时就已建立，并且不会发生改变。因此，在为测试进程分配页表时，可以简单地将已有的内核页表完整地拷贝到用户页表中，随后在此基础上进行用户页表的映射工作。在映射内核页表时，页表项的U位已被关闭，这确保了用户空间无法访问内核空间，从而有效地保证了内核空间与用户空间之间的隔离。
\end{enumerate}



\section{缺页和按需调页}

任务一要求在内核初始化时，静态设置好所有虚拟地址向物理地址的转换方式。但Linux等操作系统中常常采用延迟、按需调页的机制，只有在某一虚拟页面上的数据真正被使用时，才建立地址翻译映射。
缺页异常可能发生在以下两种情况下：首先，当软件访问的虚拟地址尚未建立相应的虚拟地址与物理地址之间的映射时，就会导致缺页异常。其次，即使已建立了虚实地址映射，如果对应的物理页框并未在物理内存中，而是被换出到了外部存储介质（如硬盘）上，也会引发缺页异常。这两种情况都表明系统无法找到所需的物理内存页面，因此需要采取相应措施来处理缺页情况。

在任务二中，我们将继续完善内存管理机制，实现按需调页功能，并针对未建立映射的缺页情况实现相应的缺页处理程序。从这一部分开始的内容，仅需准备做到A-core和C-core的同学完成。这一过程将涉及对缺页异常的捕获和处理，确保系统能够有效地加载所需的页面，并维护进程的正常运行。通过实现这一机制，我们将进一步提高系统的内存利用效率和响应能力。


\subsection{缺页 (page fault)}

当读/写指令访问的地址无法被成功翻译成物理地址时（即出现页表项的缺失、损坏），CPU会自动触发读/写缺页异常，重定向到异常处理入口。在识别与特权等级对应的\texttt{Xcause}寄存器后，操作系统能够执行对应的缺页处理程序。触发缺页异常的地址会存储在\texttt{stval}寄存器中。

缺页处理程序的主要任务是建立从虚拟页面到物理页面的映射，并将该映射加入到页表中。这一过程将在任务二中实现。如果发现该映射已存在，但其对应的页面位于磁盘上，此时则需要进行页替换。有关页替换的处理，将作为任务三的内容继续探讨。


\subsection{按需调页 (On-demand Paging)}

在之前的任务中，我们在初始化内核时便为进程建立了所有需要的页表项，并设置了虚拟地址与物理地址之间的映射。然而，在实际系统中，一个进程所需的资源并不需要在初始化时全部分配，而是可以根据实际需求在程序真正使用时再进行分配，这便是按需调页机制。这种机制的引入使得初始化过程更加简化，同时也能够根据程序的实际使用情况来有效管理内存资源，按需调页因而成为操作系统中的一种常见且重要的机制。

在本任务中，同学们需要实现按需调页功能。在上一个实验中，我们为包括程序加载空间和用户栈空间在内的一大块区域分配了内存，然而实际上大部分内存并未被使用，造成了资源的浪费。所以，在这一任务中，每当程序访问一个虚拟地址时，如果该地址尚未被分配物理页面，那么系统将自动为其分配一个。这样，程序的初始化只需为实际需要的一部分页面进行分配，以提高内存利用效率。


\subsection{任务二：动态页表和按需调页}

自行实现缺页处理程序，动态建立页表映射，分配物理页框。需通过下述测试用例：

\begin{enumerate}
    \item 使用\texttt{fly}和\texttt{rw}两个程序作为测试用例，其中\texttt{rw}会接受命令行参数，并读写相应的地址。例如：

\begin{lstlisting}[language=bash]
exec rw 0x10800000 0x80200000 0xa0000320
\end{lstlisting}
    
    若执行上面的命令，\texttt{rw}程序会访问\texttt{0x10800000}、\texttt{0x80200000}、\texttt{0xa0000320}这三个虚拟地址，并向其中写入一个随机数。如果所有地址均能顺利写入，并读出相同的数据，则程序测验通过。\texttt{rw}程序参数中几个虚拟地址的值应在测试的时候被随机指定，且确保在用户态地址空间内。
    
    \item 使用 \texttt{test/test\_project4/lock.c} 作为测试用例，其功能与 Project 2 中的同名程序一致。
    使用 \texttt{exec lock [行号] \&} 在shell中启动多个\texttt{lock}测试，任何时候都只有一个进程能获取锁并打印相关提示信息。
\end{enumerate}


\section{换页机制（page swap）}

在完成了前两个任务之后，我们已经可以使用全部的内存空间，拥有了真正的进程，并实现了按需调页机制。
而本任务则包括了换页机制 (swap) 和页替换算法，这些功能将使虚拟内存的管理更加完整。

换页机制是指当系统的物理内存不够用时，将部分虚拟页面对应的内容写入磁盘的swap空间，在需要访问时再加载回内存的机制。换页机制使得虚拟地址空间可以大于物理地址空间，从而程序员可以不用担心物理内存的大小直接使用虚拟地址。因为当虚拟地址空间可能大于物理地址空间时，一次对虚地址的访问发生了缺页中断就有两种可能：一是这个虚拟地址第一次被访问，没有分配过物理地址；二是这个地址对应的物理页面被写入了磁盘的swap空间，需要从磁盘中读回到物理内存中。因此当换页机制打开时，我们需要额外的数据结构来表示，一个虚拟页面对应的数据是否被换入到了磁盘中，被换到了磁盘的哪个位置上。

\subsection{任务三：换页机制和页替换算法}

在本实验中，我们使用SD卡上的一块空间来当作系统的swap空间，通过读写SD卡的方式来进行换页操作。SD读写使用BIOS调用实现，我们已经提供给大家。这里需要再次提醒，我们提供的SD卡读写调用需要大家填写的是物理地址作为参数。

注意：在换页机制执行时，除了需要设计一套页标识与管理机制外，也需要替换算法决定对哪一个物理页框进行替换。在本任务中，请同学们自行设计用于实现换页的数据结构与管理机制，并设计一个替换算法（可以参考操作系统理论课介绍的算法，如Clock、FIFO等），思考物理页应该如何索引、如何替换，实现相应的替换算法。

测试用例请同学们自行思考如何有效验证。一种可行的测试用例供参考：限制能使用的物理页框个数（即物理地址空间），但实际可用的虚拟地址空间大于物理地址空间，编写一个程序对该虚址范围进行可控的访问序列，从而可以触发页替换。这个访问操作可以类似之前任务二中的\texttt{rw}程序，通过输入虚地址来控制访问序列。如果要测试一些复杂的替换算法，访问序列中需要体现对热点页面的访问。在测试时，同学们最好也通过改变访存序列，来验证替换算法是否正确实现。

回到我们的换页机制，有了SD卡的读写之后，我们就可以在需要分配一个物理页框但现有物理页框不够使用时，通过页替换的方式选择一个物理页框将其写回SD卡，从而释放该物理页框用于新的分配需求；或者虚实映射已经建立好但物理页框被换出至磁盘，通过页替换的方式将页框重新换入物理内存。当然，换回物理内存的页框不一定拥有和之前一样的物理地址。

\subsubsection{注意事项}

\begin{enumerate}
    \item 在QEMU上调试本任务时，当SD卡读写的范围超过镜像大小时将会报错。建议在本任务中制作完成镜像后，在后方padding一些空间以方便QEMU上SD卡的读写。可以采用命令：
    
\begin{lstlisting}[language=bash]
dd if=/dev/zero of=image oflag=append conv=notrunc bs=64MB count=1
\end{lstlisting}
    
    该命令表示在镜像\texttt{image}后方padding一块大小为64MB的空间，为了方便，建议大家将该命令写入到\texttt{Makefile}中。padding操作会减缓SD卡写入速度，同学们可手动减小padding大小以节约时间。\textbf{此外，如果64MB不够用，请自行调整padding的大小。}
    
    \texttt{请同学们再次确认包括镜像文件在内的\texttt{build}目录已经被加入到\texttt{.gitignore}中并且不再参与Git同步，否则会导致巨大的存储浪费进而带来不必要的麻烦。}

\end{enumerate}

%注意：系统触发缺页中断时，如果需要进行换页，则会进行磁盘I/O，这将是一个比较耗时的操作。
%因此，一个程序在进入换页操作时，应该将自己挂起（block），等待换页动作完成。
%但是由于我们提供的sbi接口对磁盘的读写没有DMA模式，在sbi函数执行完成时保证数据已经读写完毕，所以我们在实现时并不需要将进程挂起。



\section{多线程管理}

有了虚拟内存机制之后，我们可以完全区分开进程和线程的概念，而不用像之前那样模糊两者的区别。两个进程会拥有完全独立的虚拟地址空间，从而自然的形成数据的隔离保护。而两个线程则应该拥有完全相同的虚拟地址空间，自然的形成数据共享。只是两个线程可以执行不同的代码，可以由内核分别调度。任务四需要同学们实现线程这一机制，并完成一个多线程异步收发的mailbox测试。mailbox原本是 Project 3 中利用多进程并发完成的，然而现在多线程并发也能做到相同的事情，并且编程难度可能更低。

\subsection{任务四：多线程的mailbox收发测试}

本任务中，同学们需要完成对下述操作的支持：开启三个进程，其中每个进程建立两个线程，分别执行发送mail和接收mail的操作。start code 内的测试程序代码为 \texttt{test/test\_project4/mailbox.c}。

同学们需要实现的是线程相关的功能和接口，包括了测试程序中用到的\texttt{pthread\_create()}和\texttt{pthread\_join()}，以保证 Project 3 实现的mailbox相关系统调用功能正确。\texttt{pthread\_create()}是创建一个线程，执行指定的代码。创建完成后原线程继续执行其他的代码，而新创建出来的线程执行\texttt{pthread\_create()}指定的代码。这样就实现了双线程执行不同的代码。线程虽然由内核分别调度执行，但是由于共享相同的虚拟地址空间，所以都可以访问进程内的所有数据。

一个mailbox进程内，主线程执行主函数，调用\texttt{pthread\_create()}函数创建了一个接受线程，它的线程描述符为\texttt{recv}，执行\texttt{recv\_thread()}函数，传递\texttt{id}作为参数。创建完这个线程之后，主线程执行\texttt{send\_thread()}并化身为发送线程，随机向另外两个mailbox进程之一发送接收线程所收到的字符串。也就是说，两个线程需要通过共享一个buffer来传递这一字符串。\texttt{recv\_thread()}和\texttt{send\_thread()}中，对\texttt{sys\_mbox\_send()}、\texttt{sys\_mbox\_recv()}、\texttt{sys\_mbox\_open()}等mailbox功能函数的调用均与 Project 3 实现的功能相同。

\texttt{main()}函数最后还调用了\texttt{pthread\_join()}函数，阻塞等待\texttt{recv}线程执行结束，回收线程资源。
%由于我们测试程序的特殊性，\texttt{main()}函数应该不会执行到这里，recv线程应该也不会结束。
%所以这个函数的功能同学们可以简化或暂时不实现。但是严谨的话还是希望同学们实现好线程回收的机制。

\subsubsection{注意事项}

\begin{enumerate}
    % \item 由于之前在Project 2中做C-core的同学就已经做过线程相关的实验并已经设计了相应的接口。为了避免覆盖冲突，我们本次的patch没有将start\_code框架中的线程相关的文件拷贝，请大家自行参考patch中的
    % startfull文件夹中对应的pthread.c和pthread.h文件进行修改。
    
    \item 为了让操作系统感知并调度进程所创建的线程，\texttt{pthread\_create()}函数势必要进行系统调用，令内核创建相关的数据结构（类似于PCB的进程控制块 (TCB, Thread Control Block)），并将其加入调度队列。
    但是和进程不同的是，线程不需要单独的根页目录，只需要使用与父线程相同的地址翻译映射。
    同一进程的两个线程之间任务切换时，也不需要进行页表切换（亦或者是切换到了相同的根页目录上）。
    
    \item 两个线程依然需要独立的栈空间，以独立的运行代码和控制各自的临时变量。
    
    \item 再次请大家注意内核初始化的时候\texttt{sstatus}寄存器的置位，其中\texttt{SUM}位控制着Supervisor态下的程序是否可以访问U位为1的页面。请大家确认\texttt{SUM}位已经被置为1，这样mailbox的一些需要内核把数据拷贝到用户空间的操作才不会触发例外。
    
    % \item 在Project 2的时候做过C-core的同学在那个时候已经实现过一个线程的机制，请同学们思考，在有和没有虚拟内存机制的情况下，线程的实现有哪些区别。
\end{enumerate}

\begin{note}
    此处我们实现的是一个较为简单的线程管理机制，\texttt{pthread\_create()}使用的系统调用只需要参考\texttt{do\_exec()}进行实现即可。实际的pthread库还涉及到信号等更加复杂的管理机制，同时需要使用功能更为复杂的\texttt{do\_clone()}函数（该函数也用于fork），可以自行查阅相关资料了解。
    
    本任务需要允许Supervisor态下的程序访问U位为1的页面，但这明显是不安全的操作。如果同学们感兴趣，可以了解\texttt{copy\_to\_user()}和\texttt{copy\_from\_user()}的实现，思考如何对这个问题进行改善。
\end{note}


% 2024 Spring
% \section{虚拟内存机制下的功能集成}

% 该任务只需要 C-core 的同学完成。


% \subsection{任务五：虚拟内存机制下的网卡驱动与文件系统}

% 该任务需要将大家之前实现的网卡驱动以及文件系统适配到虚拟内存机制当中，需要大家自行添加网卡以及文件系统中的测试文件。


% \subsubsection{注意事项}

% \begin{enumerate}
% \item 在内核中访问用户地址空间时，时刻关注 \texttt{sstatus} 寄存器中 \texttt{SUM} 位的置位。

% \item 内核中的地址操作对象只能是虚拟地址。

% \item SD卡读写相关的函数传入的地址参数是物理地址。

% \item 在网卡实验中，开启了虚拟内存时，将无法直接访问物理地址。因此，在这种情况下，需要将物理地址映射到内核段的虚地址上，通过虚地址进行访问。在Linux中，这一操作叫做ioremap，也就是将一个物理地址映射到某个空闲的虚地址上。在我们的实验框架中提供了ioremap函数，在之前的网卡实验中，其直接返回传入的物理地址参数。该该任务中需要实现其完整的功能，根据传递的物理地址以及长度信息，将对应的物理地址映射到内核虚拟地址上并返回对应的内核虚拟地址。

% \item 在网卡实验中，ioremap映射的地址是内核地址，也就是说只有内核才能够访问，由于我们目前的 OS 实现不支持嵌套中断，所以需要像建立内核页表一样将页表项的\texttt{A/D}位都拉高。同时，网卡实验当中的\texttt{do\_net\_send}和下文将会提到的\texttt{do\_net\_recv}函数直接在内核中访问用户态地址。为了避免在内核当中发生例外，因此需要将页表项的\texttt{A/D}位都置上。总的来说，就是大家建立页表的时候，需要将所有页表项的\texttt{A/D}位都置为1。

% \item 在网卡实验中，填写到发送以及接受描述符中的 buffer 地址只能是 \textbf{物理地址}；DMA 也只认\textbf{物理地址}。

% \end{enumerate}



\section{内存的共享与保护}

目前同学们实现的操作系统中，不同进程不允许使用各自的虚地址访问同一块物理地址，但同一进程的虚拟地址只要特权级满足，几乎可以被任意访问。下面的实验中，同学们需要尝试为自己的操作系统添加一些进程内存地址空间的共享和保护机制，以提升效率、保障安全。从这里开始的部分只需要C-core的同学完成。

\subsection{共享内存的概念和用处}

共享内存可以允许两个进程将各自的虚页映射到同一个物理页面上，这样两个进程就可以简便的修改同一个内存数据。
从内核的角度来说，内核需要在用户进程申请共享内存时提供这样的功能，建立好虚实映射，并管理这个物理页的回收。
在有进程释放了映射到共享物理页的虚拟页时，如果有其他进程依然在使用这个物理页，则这个物理页不能被回收。
直到所有进程都释放了到这个物理页的映射，这个物理页才可以被回收。
当然，同时修改共享内存的内存数据就存在操作的原子性问题，因此我们在操作共享数据的时候就需要使用原子指令进行操作。


\subsection{任务五：多进程共享内存与进程通信}

请大家参考测试程序完成本任务，测试程序为\texttt{test/test\_project4/consensus.c}。

多核共享内存需要实现两个接口：

\begin{description}
    \item[\texttt{sys\_shmpageget(int key)}] 该接口的输入是一个key值，同样的key值会对应到同一块共享内存区域上。返回值是一个地址。这个地址是由内核寻找的一块尚未被使用的虚拟内存区域。由内核将共享的内存映射到该地址上，并将地址返回。如果一个key是第一次被使用，内核会建立一个全0的空白物理页面作为共享页面。后续传递同样的key的进程得到的物理页面是同样的。从而实现共享内存。
    \item[\texttt{sys\_shmpagedt(void* addr)}] 该接口将之前用shmpageget获取到的虚地址解除与共享内存区域的映射。如果没有进程再使用对应的共享内存时，需要将物理页回收。
\end{description}
    
测试程序启动后会首先测试共享内存的获取与回收。
在获取到共享内存的虚地址后，会首先立即解除映射，然后尝试对之前获取到的虚地址进行写入。
由于映射已解除，所以写入操作会导致内核为该虚地址自动分配一个新的物理页。
然后测试程序会再次尝试获取共享内存。由于之前的虚地址已经被占用，所以内核只能返回一个新的虚地址供共享内存使用。
这样，同学们实现的shmpageget就能保证每次返回的虚拟页都是动态寻找的空闲虚拟地址，而不是每次静态指定的虚地址。

在完成上述测试后，测试程序会创建一定数量的子进程。创建子进程的过程依然使用的是exec接口，请同学们参见测试程序。
所有子进程会获得到同一块共享内存。之后这些进程会通过共享内存和原子指令，每一轮选择一个进程号。
被选中的进程会显示自己退出了（虽然并未实际退出），其他进程会显示本轮被选中的是哪个进程。
之后，每一轮都会选择一个进程，直到所有的进程全部被选过一遍为止。
在这一过程中，所有的进程都需要正确显示被选中的进程是哪一个。当所有进程都被选择之后，所有进程一起退出。
测试程序的输出效果如图\ref{fig:shmpage}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{shmpage-result}
    \caption{共享内存测试效果}
    \label{fig:shmpage}
\end{figure}


\subsection{内存保护：\texttt{mprotect()}}

Linux操作系统中有很多跟内存相关的系统调用，如\texttt{mprotect()}、\texttt{brk()}、\texttt{sbrk()}、\texttt{mmap()}、\texttt{munmap()}等等。本任务中，我们想要让同学们实现\texttt{mprotect()}系统调用的功能。

\subsection{\texttt{mprotect()}系统调用}

\begin{note}  \rmfamily\sffamily
       mprotect() changes the access protections for the calling
       process's memory pages containing any part of the address range
       in the interval [addr, addr+len-1].  addr must be aligned to a
       page boundary.

       If the calling process tries to access memory in a manner that
       violates the protections, then the kernel generates a SIGSEGV
       signal for the process.

       prot is a combination of the following access flags:
       
       PROT\_NONE or
       a bitwise OR of the other values in the following list:

       PROT\_NONE
              The memory cannot be accessed at all.

       PROT\_READ
              The memory can be read.

       PROT\_WRITE
              The memory can be modified.

       PROT\_EXEC
              The memory can be executed.
              
        ... \cite{linux-mprotect}
\end{note}

\texttt{mprotect()}是一个系统调用，用于在Unix和Linux操作系统中更改一个进程的内存保护属性。具体来说，它允许开发者设置或修改一个内存区域的访问权限，如可读、可写或可执行。这个系统调用的函数原型如下：

\begin{lstlisting}[language=c]
int mprotect(void *addr, size_t len, int prot);
\end{lstlisting}

对这个系统调用的行为说明如表\ref{tab:mprotect_para}所示。

\begin{table}[hbtp]
    \centering
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        参数/返回值  &  说明   \\
        \midrule
        \texttt{addr} & 要修改权限的内存区域的起始地址。    \\
        \texttt{len}  & 该内存区域的长度（以字节为单位）。  \\
        \texttt{prot} & 新的内存保护标志，可能的值包括
                        \texttt{PROT\_NONE}（禁止访问），
                        \texttt{PROT\_READ}（允许读访问），
                        \texttt{PROT\_WRITE}（允许写访问）和
                        \texttt{PROT\_EXEC}（允许执行访问）。\\
        返回值        & 成功时返回\texttt{0}，失败时返回\texttt{-1}。\\
        \bottomrule
    \end{tabularx}
    \caption{\texttt{mprotect()}系统调用参数、返回值详细说明}
    \label{tab:mprotect_para}
\end{table}

\texttt{mprotect()}常用于多个场景，包括在实现内存映射文件或共享内存时，根据需要调整内存区域的权限。此外，在动态生成代码的应用程序中，它允许某些内存区域被标记为可执行。同时，\texttt{mprotect()}也能够增强安全性，防止某些内存区域被意外修改或执行。通过合理使用\texttt{mprotect()}，程序可以更灵活地管理内存保护，从而提升安全性和性能。

\subsection{任务六：\texttt{mprotect()}系统调用}

本实验中，同学们需要实现一个自己心目中的\texttt{mprotect()}系统调用。这个系统调用需要满足几点要求：

\begin{enumerate}
    \item 遵守上述关于系统调用名称、参数、返回值的定义。
    \item \texttt{addr}应与页边距对齐，否则认为系统调用执行失败。
    \item 若在虚拟内存管理开启时调用，保护此进程一切与\texttt{[addr, addr+len-1]}有交集的虚拟页面。
    \item 对受保护的页发生超出限制的请求时，报出\texttt{mprotect()}相关的错误信息。
    \item 对虚拟地址空间的与线程兼容，即同进程的不同线程所受保护一致。
\end{enumerate}

本实验并不设置固定的测试程序。请同学们参考任务书与网络资料，自行设计实现\texttt{mprotect()}系统调用，设计相关测试程序，全面测试\texttt{mprotect()}的功能。


\subsection{内存“扩容”：\texttt{brk()/sbrk()}}

\begin{note}  \rmfamily\sffamily
       brk() and sbrk() change the location of the program break, which
       defines the end of the process's data segment (i.e., the program
       break is the first location after the end of the uninitialized
       data segment).  Increasing the program break has the effect of
       allocating memory to the process; decreasing the break
       deallocates memory.

       brk() sets the end of the data segment to the value specified by
       addr, when that value is reasonable, the system has enough
       memory, and the process does not exceed its maximum data size
       (see \href{https://man7.org/linux/man-pages/man2/setrlimit.2.html}{setrlimit(2)}).

       sbrk() increments the program's data space by increment bytes.
       Calling sbrk() with an increment of 0 can be used to find the
       current location of the program break. \cite{linux-brk}
\end{note}

本任务中，我们想要让同学们实现\texttt{brk()/sbrk()}两个系统调用，并在用户程序的库函数\texttt{tiny\_libc}中添加\texttt{malloc()}与\texttt{free()}这对内存分配函数，动态分配进程的堆空间，以供用户使用。

程序的break（断裂点）是其数据段的结尾地址，也是程序未初始化数据的起始位置。\texttt{brk()}和\texttt{sbrk()}这两个系统调用会更改一个程序（也是一个进程）break的位置；增大break能够增大程序拥有的内存空间，反之则会释放内存。在系统有足够内存空间且进程未超出其最大数据大小时，\texttt{brk()}能够将break设置成参数指定的合理地址。\texttt{sbrk()}则是为break提供了一个相对于原先地址的增量，令其提升一定大小。这两个系统调用的函数原型如下：

\begin{lstlisting}[language=c]
int brk(void *addr);
void *sbrk(intptr_t increment);
\end{lstlisting}

声明里类型\texttt{intptr\_t}的定义在\texttt{stdint.h}文件中。对这两个系统调用的行为说明如表\ref{tab:brk_sbrk_para}所示。

\begin{table}[hbtp]
    \centering
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        参数/返回值  &  说明   \\
        \midrule
        \texttt{addr} & 欲将break设置成的地址。    \\
        返回值        & 成功时返回\texttt{0}，失败时返回\texttt{-1}。\\
        \midrule
        \texttt{increment} & 欲令break增长的空间大小。    \\
        返回值        & 成功时返回旧的break，失败时返回\texttt{(void *) -1}。\\
        \bottomrule
    \end{tabularx}
    \caption{\texttt{brk()}与\texttt{sbrk()}系统调用参数、返回值详细说明}
    \label{tab:brk_sbrk_para}
\end{table}

在实现这两个系统调用的行为后，我们在实验中要求同学们禁止用户程序对break以上的页面（不含break所在页）的访问，直到接近用户栈空间为止，用户栈所掌握的的具体大小由同学们自行把控。同时，也不允许用户程序访问距离栈底过远的内存空间，认为是栈溢出。如果尝试对这些页面进行访问，操作系统应当给出诸如 segment fault、stack overflow 的错误信息，具体错误信息请由同学们自行设计。

在报错信息完善之后，同学们自然就能在库函数中利用\texttt{brk()}或\texttt{sbrk()}系统调用，在程序数据段之后留出一个能进行动态分配的堆空间。为此，同学们需要尝试实现\texttt{malloc()}与\texttt{free()}这一堆库函数。两个函数的原型如下：

\begin{lstlisting}[language=c]
void *malloc(size_t size);
void free(void *_Nullable ptr);
\end{lstlisting}

我们要求\texttt{malloc(size)}能够在分配堆上一段\texttt{size}大小连续空间，并返回其起始地址，要求\texttt{free(ptr)}能回收\texttt{ptr}作为起始地址所对应分配的那片空间。若堆空间不足，\texttt{malloc()}应当进行\texttt{brk()/sbrk()}系统调用，扩大堆空间；使用\texttt{free()}回收的堆空间，应当有被\texttt{malloc()}函数再次分配的机会。

\subsection{任务七：\texttt{brk()/sbrk()}系统调用与内存分配库函数}

本任务中，同学们需要将\texttt{brk()/sbrk()}实现为系统调用（尽管它在其他知名操作系统内可能并非系统调用），实现库函数\texttt{malloc()/free()}。编写库函数时可以参考其他设计进行。这两个系统调用和两个库函数应满足几点要求：

\begin{enumerate}
    \item 遵守上述关于系统调用名称、参数、返回值的定义。

    \item \texttt{addr}应为合理地址，且要求\texttt{increment}必须为正值。

    \item 在执行完\texttt{crt0.S}中的初始化程序后，break应当指向载入内存的整段程序之后。
    由于编译工具链本身会将.bss段放到程序数据的最后，所以如若在.bss的初始化过程中调整break到该段的末尾，那么未初始化区域中应不含任何程序数据。
    总之，这是的break应该同时是堆空间的开始与结束处，且break可不断增长。

    \item 合理的break不应大到与用户栈空间相干扰，也须大于.bss段的末尾。应当为进程限制最大的程序数据大小，即限制break的最大值。
    
    \item 实现部分用户态无权限内存空间访问的报错。如无其他规定，目前允许访问的地址仅有：程序数据区域（高于程序入口地址，低于堆空间）、堆空间（低于break）、用户栈空间（需设置进程可使用的最大栈空间大小）。
    
    \item 修改之前任务中线程库的实现，利用堆为线程分配地址空间。

    \item \texttt{malloc()/free()}操作的空间应位于堆空间中，并遵守任务书内提到的一些规则。

    \item \texttt{free()}回收的空间应有被再次分配的机会。
\end{enumerate}

提示一下，关于\texttt{malloc()/free()}库函数设计，建议大家去学习Dennis M. Ritchie等人的著作《C程序设计语言》第8.7节的内容，在标明出处的条件下可以直接使用该实例程序（可参考XV6的源代码\texttt{user/umalloc.c}）。

如之前已完成任务四，请重新进行任务四的测试。
除此之外，本实验并不设置固定的测试程序，请先利用已有测试程序进行测试，若部分测试无法通过，请指出相关原因或对测试例进行修改。
另外，请同学们参考任务书与网络资料，设计相关测试程序，全面测试其相关功能。



% 2023 秋

% \subsection{任务六：fork和copy-on-write机制}

%在上一个任务中，我们实现了多进程共享页面，而在本任务中，将会介绍copy-on-write机制和Snapshot（快照）技术的概念。

% 在实现共享内存机制时，我们让两个进程将各自的虚页映射到同一个物理页面上，此时我们的主要目的是实现进程间通信和数据共享。
% 这时，对优化敏锐的同学可能会想到，既然我们能够让一个物理页对应两个虚页，那么我们能不能利用这个机制，
% 在两个虚页的内容完全相同（例如需要将一个已分配物理页的虚页上的内容拷贝到一个未分配物理页的虚页上）时通过修改页表项将它们映射到同一个物理页上，
% “假装”我们分配了两个物理页，从而实现对物理页的节省呢？实际上，这样做是完全可以的，这就要引出copy-on-write的概念。

% 2023 秋
% copy-on-write即写时拷贝机制是操作系统中常用的一个性能优化机制。当使用copy-on-write时，两个进程各自的一个虚页（假设为A和B）指向了同一个物理页，并且这两个虚页都被置为只读。如果其中一个进程想要修改虚页A，那么就会触发page fault，操作系统会为虚页A分配一个新的物理页面，并把虚页A原本对应的物理页中数据拷贝过去。拷贝完成后再对这段数据内容进行更改，从而避免数据冲突，又降低了数据拷贝开销（因为某些页面永远不会更改，因此不需要再分配空间）。
% ，可以先只复制它的页表项，并留下标记；
% 对有该标记的虚页，当我们需要读取其中的内容时，可以直接到已有的物理页上进行读取，此时，所有副本的页表都指向正本的物理页，
% 实际只需要占用一个物理页；
% 而在写入时，先进行物理页的分配与内容拷贝，再在新分配的物理页上进行修改，此时，一开始的拷贝操作才实际占用两个物理页。
% （为了实现写入时拷贝的功能，可以在复制页表项时擦除页面的写权限位（W位），此时对该页面的写入会触发例外，在此时即可进行如上所述的例外处理操作）
% 可以先将该页面设置为只读，当真正需要向该虚拟页面中写入数据时才进行真实的分配
% 在拷贝操作中采用该机制后，在只需要读取副本时甚至不需要新分配任何物理页，一方面可以极大地节省空间开销，另一方面也节省了多次拷贝产生的CPU和内存资源占用，可以大幅提升系统性能。

% 在实际操作系统中，copy-on-write机制最能发挥作用的地方是fork(clone)系统调用，该调用需要拷贝进程的全部地址空间，正适合使用copy-on-write机制。
% 因此本任务需要同学们自己实现一个fork机制，以及配套的测试程序，以正确演示fork的效果。这里有一个附加的要求，请同学们不要在主函数里直接调用fork，必须先调用一个其他函数，在函数中再使用fork系统调用。添加这个要求的原因是，fork之后包括堆栈的内容也需要进行复制，而堆栈中可能包含了函数的返回地址，而如果调用fork时堆栈内容比较简单，就难以体现出堆栈复制的重要性。值得一提的是，如果在没有虚拟内存的条件下使用fork，在物理地址上复制了堆栈内容之后，可能还需要根据新进程的物理地址情况，去修改堆栈里面保存的地址，但是有虚拟内存的情况下，两个进程可以使用相同的虚拟地址，给fork过程中的copy动作带来了方便。
%在本任务中，我们让它在另一个用武之地发挥价值，即Snapshot技术。Snapshot（快照）技术主要是在操作系统以及存储技术上实现的一种记录某一时间系统状态的技术，其定义是：“关于指定数据集合的一个完全可用拷贝，该拷贝包括相应数据在某个时间点(拷贝开始的时间点)的映像”，具有瞬时、完整、快速恢复等优秀特性，主要用于高可靠性文件系统管理，以及操作系统的备份和恢复。目前主流的Snapshot技术中，为了在备份数据未被改动的情况下节省空间和资源占用，copy-on-write机制被广泛使用。
% 该技术在操作系统理论课中有详细的介绍，细心的同学也会在自己使用的操作系统中发现它的身影，在此就不加赘述了。该技术在操作系统理论课的文件系统部分会有介绍。
%在本任务中，我们希望同学们自主实现copy-on-write机制，并设计一个使用Snapshot技术的测试程序进行验证。
%这里给出一种参考实现方式：
%\begin{enumerate}
%  \item 初始化分配一个虚拟页面，并在其中填入一系列已知的数据，用作正本。
%  \item 使用copy-on-write技术，定时在进程地址空间中其他未被使用的虚拟页面上创建该虚拟页面的快照，在某几个快照间隔中，随机对该页面上的某几个数据进行修改。
%  \item 创建了一定数量的快照（如10个）后，对比产生的所有快照与此时的正本，能够发现有部分快照是完全一致的（间隔内没有对数据进行改动），所有快照的内容和预期相符。%（可以使用for循环，将不同快照对应数据相减，打印结果验证）
%  \item 设计一个查询虚拟地址映射的物理页面地址的系统调用，打印这些快照和正本对应页面的用户虚地址与物理地址，能够发现，相同内容的快照页面对应的用户虚地址不同，但物理地址是相同的。
%  \end{enumerate}

%我们也鼓励同学们自行设计使用更加直观的方式进行功能的验证。

\putbib[guideref]
\end{bibunit}